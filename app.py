import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.pydantic_v1 import Field
from pydantic import BaseModel
from langgraph.graph import StateGraph
from typing import List, Annotated, Literal, Sequence, TypedDict
from langgraph.graph import END, StateGraph, START
import asyncio
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.schema import Document
from dotenv import load_dotenv
from langsmith import Client
import streamlit_authenticator as stauth
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
import japanize_matplotlib 
import numpy as np
import seaborn as sns
import json
from st_login_form import login_form
from datetime import datetime
from janome.tokenizer import Tokenizer
from wordcloud import WordCloud
from google.oauth2 import service_account
import uuid
import time
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from googleapiclient.errors import HttpError
import openai
from langchain.chat_models import ChatOpenAI
import re
from docx import Document
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.preprocessing import MultiLabelBinarizer
import plotly.graph_objects as go
from google.cloud import vision
from pdf2image import convert_from_path
import tempfile
from io import BytesIO
from PIL import Image
import cv2
from langdetect import detect
import openpyxl
import io
from langchain.vectorstores import FAISS
from langchain.schema.document import Document as LangChainDocument




# ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ã®æŒ‡å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦åˆ©ç”¨ï¼‰
font_path1 = "./font/NotoSansJP-Regular.ttf"

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
load_dotenv(dotenv_path=".env.example")

# JSONæ–‡å­—åˆ—ã‚’ä½¿ã†å ´åˆï¼ˆæ³¨æ„ï¼ševalã‚„json.loadsãŒå¿…è¦ï¼‰

SERVICE_ACCOUNT_INFO = st.secrets["GOOGLE_DRIVE_CREDENTIAL_JSON"]
VISION_ACCOUNT_INFO = st.secrets["GOOGLE_APPLICATION_CREDENTIALS"]

# Google APIèªè¨¼ï¼ˆJSONæ–‡å­—åˆ—ã®å ´åˆï¼‰
creds = service_account.Credentials.from_service_account_info(
    SERVICE_ACCOUNT_INFO,
    scopes=['https://www.googleapis.com/auth/drive']
)
drive_service = build('drive', 'v3', credentials=creds)

# Vision API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®š
oS_creds = service_account.Credentials.from_service_account_info(
    VISION_ACCOUNT_INFO
)
vision_client = vision.ImageAnnotatorClient(credentials=oS_creds)

# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
PASSWORD = os.getenv("PASSWORD")
PDF_input_folder = os.getenv("PDF_input_folder")
DOCS_output_folder = os.getenv("DOCS_output_folder")
GOOGLE_DRIVE_FOLDER_ID = os.getenv("GOOGLE_DRIVE_FOLDER_ID")
#USER_AGENT = os.getenv("USER_AGENT")
#os.environ['USER_AGENT'] = USER_AGENT

# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›ãƒœãƒƒã‚¯ã‚¹
inputText_A = st.text_input('ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„', type="password")

# ç’°å¢ƒå¤‰æ•°`PASSWORD`ã¨æ¯”è¼ƒ
if inputText_A == PASSWORD:
        
    # ---- ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸå¾Œã«è¡¨ç¤ºã™ã‚‹ã‚¢ãƒ—ãƒªã®ãƒ¡ã‚¤ãƒ³æ©Ÿèƒ½ã‚’ã“ã“ã«è¨˜è¿° ----
    st.write("ã“ã“ã«ãƒãƒ£ãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ ã‚„ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¢ãƒ¼ãƒ‰ãªã©ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè£…ã§ãã¾ã™ã€‚")

    # --- ãƒ¢ãƒ¼ãƒ‰é¸æŠ ---
    mode = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰é¸æŠ", ( "ç‰¹è¨±è¦ç´„ã‚·ã‚¹ãƒ†ãƒ ", "ãƒ‡ãƒ¼ã‚¿è§£æã‚·ã‚¹ãƒ†ãƒ ", "å¯¾æ¯”è¡¨ä½œæˆã‚·ã‚¹ãƒ†ãƒ "))

    if mode == "ç‰¹è¨±è¦ç´„ã‚·ã‚¹ãƒ†ãƒ ":
        # --- Google Drive é€£æºé–¢æ•° ---
        st.title("ğŸ“„ç‰¹è¨±è¦ç´„ã‚·ã‚¹ãƒ†ãƒ ")
        st.write("ç‰¹è¨±PDFãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆç¾æ™‚ç‚¹ã§ã¯æ—¥æœ¬èªç‰¹è¨±ã®ã¿ï¼‰ã‚’èª­ã¿è¾¼ã¿ã€æ–‡çŒ®ã®ä¸­èº«ã‚’ChatGPTãŒç¢ºèªã—ã€è¡¨ã«ã¾ã¨ã‚ã¾ã™ï¼ä¸€æ™‚çš„ã«ç§ç”¨ã®googledriveã«ä¿ç®¡ã•ã›ã¦é ‚ãã¾ã™ã®ã§ã€æ©Ÿå¯†æƒ…å ±ã¯çµ¶å¯¾ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãªã„ã§ãã ã•ã„ã€‚ã¾ã è¦ç´„ã‚¤ãƒã‚¤ãƒã‹ã‚‚ã—ã‚Œãªã„ã§ã™ã€‚")
        st.image("./fig/ç‰¹è¨±è¦ç´„ã‚·ã‚¹ãƒ†ãƒ 1.jpg", use_container_width=True)
        st.image("./fig/ç‰¹è¨±è¦ç´„ã‚·ã‚¹ãƒ†ãƒ 2.jpg", use_container_width=True)
        
        
        def upload_to_drive(folder_id, file_path, mime_type='application/pdf'):
            unique_name = f"{os.path.splitext(os.path.basename(file_path))[0]}_{int(time.time())}_{uuid.uuid4().hex[:6]}.pdf"
            media = MediaFileUpload(file_path, mimetype=mime_type)
            file_metadata = {'name': unique_name, 'parents': [folder_id]}
            uploaded = drive_service.files().create(body=file_metadata, media_body=media, fields='id,name').execute()
            return uploaded['id'], uploaded['name']

        def delete_from_drive(file_id):
            try:
                drive_service.files().delete(fileId=file_id).execute()
            except HttpError as e:
                print(f"å‰Šé™¤å¤±æ•—: {e}")  # ãƒ­ã‚°ã ã‘æ®‹ã™

        def convert_pdf_to_doc(file_id, output_folder_id):
            copied = drive_service.files().copy(fileId=file_id, body={
                'mimeType': 'application/vnd.google-apps.document',
                'parents': [output_folder_id]
            }).execute()
            return copied['id']

        def download_doc_as_docx(file_id, local_path):
            try:
                request = drive_service.files().export_media(fileId=file_id,
                                                            mimeType='application/vnd.openxmlformats-officedocument.wordprocessingml.document')
                with open(local_path, 'wb') as f:
                    f.write(request.execute())
                return local_path
            except HttpError as error:
                print(f"âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {error}")
                return None
            
        def preprocess_image_for_ocr(pil_image):
            """OCRå‰ã®ç”»åƒå‰å‡¦ç†ï¼ˆOpenCVä½¿ç”¨ï¼‰"""
            img = np.array(pil_image.convert("L"))  # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«
            img = cv2.resize(img, None, fx=2, fy=2, interpolation=cv2.INTER_LINEAR)  # 2å€æ‹¡å¤§
            img = cv2.GaussianBlur(img, (3, 3), 0)  # ãƒã‚¤ã‚ºé™¤å»
            img = cv2.equalizeHist(img)  # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆè£œæ­£
            return Image.fromarray(img)

        def extract_text_from_docx_images(docx_path):
            """docxå†…ã®ç”»åƒã‹ã‚‰OCRã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆå‰å‡¦ç†ä»˜ãï¼‰"""
            doc = Document(docx_path)
            image_texts = []

            for rel in doc.part._rels:
                rel_obj = doc.part._rels[rel]
                if "image" in rel_obj.reltype:
                    image_data = rel_obj.target_part.blob
                    pil_image = Image.open(BytesIO(image_data)).convert("RGB")

                    # â¬‡ï¸ ç”»åƒã‚’å‰å‡¦ç†
                    processed_image = preprocess_image_for_ocr(pil_image)

                    # PIL â†’ PNG â†’ bytes
                    buffered = BytesIO()
                    processed_image.save(buffered, format="PNG")
                    content = buffered.getvalue()

                    image = vision.Image(content=content)
                    response = vision_client.document_text_detection(image=image)  # â† document_text_detectionã‚’æ¨å¥¨
                    texts = response.text_annotations
                    if texts:
                        image_texts.append(texts[0].description.strip())

            return "\n".join(image_texts)
        
        def extract_text_from_docx_images(docx_path):
            """docxå†…ã®ç”»åƒã‹ã‚‰OCRã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
            doc = Document(docx_path)
            image_texts = []

            for rel in doc.part._rels:
                rel_obj = doc.part._rels[rel]
                if "image" in rel_obj.reltype:
                    image_data = rel_obj.target_part.blob
                    image = Image.open(BytesIO(image_data)).convert("RGB")

                    # PILã‚¤ãƒ¡ãƒ¼ã‚¸ â†’ bytes for Google Vision
                    buffered = BytesIO()
                    image.save(buffered, format="PNG")
                    content = buffered.getvalue()

                    image = vision.Image(content=content)
                    #response = vision_client.text_detection(image=image)
                    response = vision_client.document_text_detection(image=image)
                    texts = response.text_annotations
                    if texts:
                        image_texts.append(texts[0].description)

            return "\n".join(image_texts)
        
        translate_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        def translate_to_japanese(text):
            if not text.strip():
                return text  # ç©ºãªã‚‰ãã®ã¾ã¾
            
            try:
                # è¨€èªã‚’æ¤œå‡º
                detected_lang = detect(text)
                
                # æ—¥æœ¬èªã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
                if detected_lang == "ja":
                    return text
                    
                # æ—¥æœ¬èªä»¥å¤–ã®å ´åˆã¯ç¿»è¨³
                prompt = f"""
        ã‚ãªãŸã¯å„ªç§€ãªç¿»è¨³è€…ã§ã™ã€‚
        ä»¥ä¸‹ã®æ–‡ç« ãŒæ—¥æœ¬èªä»¥å¤–ã®è¨€èªã®å ´åˆã€è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚
        ä¸è¦ãªè¨€è‘‰(### ç¿»è¨³çµæœãªã©)ã¯é™¤ã„ã¦ã€ç¿»è¨³çµæœã®ã¿è¡¨è¨˜ã—ã¦ãã ã•ã„ã€‚
        ### ç¿»è¨³å¯¾è±¡ï¼š
        {text}
        """
                result = translate_llm.invoke(prompt)
                return result.content.strip()
            except Exception as e:
                print(f"ç¿»è¨³å¤±æ•—: {e}")
                return text  # å¤±æ•—æ™‚ã¯åŸæ–‡è¿”ã™

        def process_docx_file(docx_path, llm_model=None):
            

            # === LLMè¨­å®š ===
            llm = llm_model or ChatOpenAI(model="gpt-4o-mini", temperature=0, request_timeout=30)

            # === ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š ===
            japanese_prompt = ChatPromptTemplate.from_template("""
            ä»¥ä¸‹ã®ç‰¹è¨±æ–‡æ›¸ã‹ã‚‰ã€æŒ‡å®šã•ã‚ŒãŸé …ç›®ã‚’æ­£ç¢ºãªJSONå½¢å¼ï¼ˆPythonã®è¾æ›¸å½¢å¼ï¼‰ã§æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

            âš ï¸ çµ¶å¯¾ã«JSONå½¢å¼ã®ã¿ã‚’è¿”ç­”ã—ã¦ãã ã•ã„ï¼ˆæ–‡ç« ã‚„ã‚³ãƒ¡ãƒ³ãƒˆã‚’åŠ ãˆãªã„ã“ã¨ï¼‰ã€‚
            âš ï¸ å‡ºåŠ›å½¢å¼ã®ä¾‹ã«å¾“ã„ã€å„é …ç›®ã®å€¤ã¯å…·ä½“çš„ãƒ»æ–‡æ›¸ãƒ™ãƒ¼ã‚¹ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

            å‡ºåŠ›å½¢å¼ã®ä¾‹ï¼š
            {{
                "ç™ºæ˜ã®åç§°": "...",
                "å‡ºé¡˜äºº": "æ–‡ç« ã«åŸºã¥ã„ã¦å‡ºé¡˜äººã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚è¤‡æ•°è¡¨è¨˜å¯ï¼ˆä¾‹ï¼šãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šæ ªå¼ä¼šç¤¾;æ ªå¼ä¼šç¤¾è±Šç”°è‡ªå‹•ç¹”æ©Ÿï¼‰",
                "ç™ºæ˜è€…": "æ–‡ç« ã«åŸºã¥ã„ã¦ç™ºæ˜è€…ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚ï¼ˆä¾‹ï¼šç‰¹è¨± å¤ªéƒ)è¤‡æ•°è¡¨è¨˜å¯ï¼ˆä¾‹ï¼šç‰¹è¨± å¤ªéƒ;ç‰¹è¨± èŠ±å­ï¼‰",
                "å…¬é–‹ç•ªå·": "...",
                "å…¬é–‹æ—¥": "æ–‡ç« ã«åŸºã¥ã„ã¦å…¬é–‹æ—¥ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚è¨˜è¼‰ã®ãƒ«ãƒ¼ãƒ«ã¨ã—ã¦ã¯è¥¿æš¦è¡¨ç¤ºã«å¤‰æ›ã—ã¦ã€å…¨ã¦åŠè§’ã§è¨˜è¼‰ãã ã•ã„ã€‚(ä¾‹ï¼š2025/2/6)",
                "è¦ç´„": "èª²é¡Œã€ç‰¹è¨±è«‹æ±‚ã®ç¯„å›²ã€ç™ºæ˜ãŒè§£æ±ºã—ã‚ˆã†ã¨ã™ã‚‹èª²é¡Œã€ç™ºæ˜ã®åŠ¹æœã«é–¢ã™ã‚‹å†…å®¹ã‚’300æ–‡å­—ä»¥ä¸‹ã§ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚",
                "å¯¾è±¡ç‰©": "ç™ºæ˜ã®åç§°ã‹ã‚‰ç‰¹è¨±ã®å¯¾è±¡ç‰©ã‚’ä¸€è¨€ã§è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚(ä¾‹ï¼šã‚¿ãƒ³ã‚¯)è¤‡æ•°è¡¨è¨˜å¯ã€‚ï¼ˆã‚¿ãƒ³ã‚¯;å£é‡‘ï¼‰",
                "å¯¾è±¡ç‰©ã®æœ€çµ‚ç”¨é€”": "æ–‡ç« ã«åŸºã¥ã„ã¦å¯¾è±¡ç‰©ãŒæœ€çµ‚çš„ã«ä½¿ç”¨ã•ã‚Œã‚‹ç”¨é€”åˆ†é‡ï¼ˆä¾‹ï¼šç‡ƒæ–™é›»æ± è»Šã€æ°´ç´ ã‚¤ãƒ³ãƒ•ãƒ©ï¼‰ã€‚",
                "è«‹æ±‚é …ã®å¯¾è±¡": "æ–‡ç« ã«åŸºã¥ã„ã¦è«‹æ±‚é …ã®å¯¾è±¡ã«ã¤ã„ã¦ä¸€è¨€ã§è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚ï¼ˆä¾‹ï¼šã‚¿ãƒ³ã‚¯ï¼‰è¤‡æ•°å¯¾è±¡ãŒã‚ã‚‹å ´åˆã¯è¤‡æ•°æ˜è¨˜ãã ã•ã„ã€‚ï¼ˆä¾‹ï¼šã‚¿ãƒ³ã‚¯;è£½é€ æ–¹æ³•ï¼‰",
                "æŠ€è¡“é–‹ç™ºã®èƒŒæ™¯ãƒ»èª²é¡Œ": "èƒŒæ™¯ã‚„èª²é¡Œã‚’å…·ä½“çš„ã«è¨˜è¿°ï¼ˆæ–‡æ›¸ã«åŸºã¥ã„ã¦ï¼‰",
                "æŠ€è¡“é–‹ç™ºèª²é¡Œåˆ†é¡": "è§£æ±ºã—ã‚ˆã†ã¨ã™ã‚‹æŠ€è¡“èª²é¡Œã‚’ä¸€è¨€ã§è¡¨ç¾ï¼ˆä¾‹ï¼šå¼·åº¦å‘ä¸Šã€å‰›æ€§å‘ä¸Šï¼‰ã€‚è¤‡æ•°è¡¨è¨˜å¯ã€‚ï¼ˆå¼·åº¦å‘ä¸Š;å‰›æ€§å‘ä¸Šï¼‰",
                "è§£æ±ºæ‰‹æ®µã®æ¦‚è¦": "èª²é¡Œã®è§£æ±ºã«å‘ã‘ã¦ã®æ‰‹æ®µã‚’å…·ä½“çš„ã«è¨˜è¿°ï¼ˆæ–‡æ›¸ã«åŸºã¥ã„ã¦ï¼‰ã€‚",
                "è§£æ±ºæ‰‹æ®µåˆ†é¡": "è§£æ±ºæ‰‹æ®µã‚’ä¸€è¨€ã§è¡¨ç¾ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šææ–™æ”¹è‰¯ã€å½¢çŠ¶æ”¹è‰¯ã€é…ç½®æ”¹è‰¯ã€è¡¨é¢åŠ å·¥ï¼‰ã€‚è¤‡æ•°è¡¨è¨˜å¯ã€‚ï¼ˆé…ç½®æ”¹è‰¯;è¡¨é¢åŠ å·¥ï¼‰",
                "å›½éš›ç‰¹è¨±åˆ†é¡": "å›½éš›ç‰¹è¨±åˆ†é¡ï¼ˆä¾‹ï¼šF17C 1/06 ï¼‰ãŒã‚ã‚Œã°æ­£ç¢ºã«ã€å…¨ã¦è¨˜è¼‰ãŠé¡˜ã„ã—ã¾ã™ã€‚è¤‡æ•°è¡¨è¨˜å¯ã€‚ï¼ˆF16J 12/00;F17C 13/00ï¼‰ãªã‘ã‚Œã°ç©ºæ–‡å­—ã€‚å›½éš›ç‰¹è¨±åˆ†é¡ã¯æ•°å­—(â–³)ã¨ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ(â—‹))ã§æ§‹æˆã€‚(â—‹â–³â–³â—‹ â–³â–³/â–³â–³ )ã€‚è¨˜è¼‰ã®ãƒ«ãƒ¼ãƒ«ã¨ã—ã¦ã¯å…¨ã¦åŠè§’ã«å¤‰æ›ã—ã¦è¨˜è¼‰ãã ã•ã„ã€‚",
                "å…ˆè¡ŒæŠ€è¡“æ–‡çŒ®"ï¼š"å…ˆè¡ŒæŠ€è¡“æ–‡çŒ®ã®è­˜åˆ¥ã‚³ãƒ¼ãƒ‰ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚å…·ä½“çš„ã«ã¯ç‰¹è¨±ã®å…¬é–‹ç•ªå·(ä¾‹ï¼šç‰¹é–‹ï¼’ï¼ï¼’ï¼‘ï¼ï¼‘ï¼•ï¼–ï¼“ï¼‘ï¼’å·)ãªã©ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚ãªã‘ã‚Œã°ç©ºæ–‡å­—ã€‚è¤‡æ•°è¡¨è¨˜å¯ã€‚ï¼ˆç‰¹é–‹ï¼’ï¼ï¼’ï¼‘ï¼ï¼‘ï¼•ï¼–ï¼“ï¼‘ï¼’å·;ç‰¹é–‹ï¼’ï¼ï¼’ï¼‘ï¼ï¼‘ï¼‘ï¼“ï¼•ï¼–ï¼å·ï¼‰ã€‚"
                "Fã‚¿ãƒ¼ãƒ ": "Fã‚¿ãƒ¼ãƒ ãŒã‚ã‚Œã°æ­£ç¢ºã«ã€ãªã‘ã‚Œã°ç©ºæ–‡å­—",

            }}

            ## ç‰¹è¨±å…¨æ–‡:
            {text}
            """)
            
            chinese_prompt = ChatPromptTemplate.from_template("""
            è¯·æ ¹æ®ä»¥ä¸‹é¡¹ç›®ï¼Œä»ä¸“åˆ©å…¨æ–‡ä¸­æå–ä¿¡æ¯ï¼Œå¹¶ä»¥**æ—¥è¯­**è¾“å‡ºï¼Œæ ¼å¼ä¸º JSONï¼ˆPython å­—å…¸å½¢å¼ï¼‰ã€‚

            âš ï¸ åªè¿”å› JSON æ ¼å¼ï¼ˆä¸è¦æ·»åŠ ä»»ä½•æ–‡å­—è¯´æ˜æˆ–è¯„è®ºï¼‰ã€‚
            âš ï¸ è¯·æŒ‰ç…§è¾“å‡ºç¤ºä¾‹çš„æ ¼å¼ï¼Œå°†å„é¡¹å†…å®¹åŸºäºæ–‡æ¡£å‡†ç¡®å…·ä½“å¡«å†™ã€‚
            âš ï¸ è¾“å‡ºå†…å®¹å¿…é¡»ä¸º**æ—¥è¯­**ã€‚
            âš ï¸ è¯·å‚è€ƒä¸‹æ–¹æ ¼å¼ç¤ºä¾‹...
            

            è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
            {{
                "å‘æ˜åç§°": "...",
                "ç”³è¯·äºº": "è¯·æ ¹æ®æ–‡ç« å†…å®¹å¡«å†™ç”³è¯·äººï¼Œå¯å¡«å†™å¤šä¸ªï¼ˆä¾‹å¦‚ï¼šä¸°ç”°æ±½è½¦æ ªå¼ä¼šç¤¾;ä¸°ç”°è‡ªåŠ¨ç»‡æœºæ ªå¼ä¼šç¤¾ï¼‰",
                "å‘æ˜äºº": "è¯·æ ¹æ®æ–‡ç« å†…å®¹å¡«å†™å‘æ˜äººï¼Œå¯å¡«å†™å¤šä¸ªï¼ˆä¾‹å¦‚ï¼šç‰¹è®¸ å¤ªéƒ;ç‰¹è®¸ èŠ±å­ï¼‰",
                "å…¬å¼€ç¼–å·": "...",
                "å…¬å¼€æ—¥": "è¯·æ ¹æ®æ–‡ç« å†…å®¹å¡«å†™å…¬å¼€æ—¥ï¼Œè½¬æ¢ä¸ºè¥¿å†è¡¨ç¤ºï¼Œå¹¶å…¨éƒ¨ä½¿ç”¨åŠè§’å­—ç¬¦ã€‚ï¼ˆä¾‹å¦‚ï¼š2025/2/6ï¼‰",
                "æ‘˜è¦": "è¯·å°†ä¸é—®é¢˜ã€æƒåˆ©è¦æ±‚ã€æ‰€è¦è§£å†³çš„é—®é¢˜ã€å‘æ˜æ•ˆæœç›¸å…³å†…å®¹æ€»ç»“åœ¨300å­—ä»¥å†…ã€‚",
                "å¯¹è±¡ç‰©": "è¯·ä»å‘æ˜åç§°ä¸­æå–ä¸“åˆ©çš„å¯¹è±¡ç‰©ï¼Œç”¨ä¸€ä¸ªè¯è¡¨è¾¾ï¼ˆä¾‹å¦‚ï¼šç½ä½“ï¼‰ï¼Œå¯å¡«å†™å¤šä¸ªã€‚ï¼ˆç½ä½“;æ¥å£ï¼‰",
                "å¯¹è±¡ç‰©çš„æœ€ç»ˆç”¨é€”": "æ ¹æ®æ–‡ç« å†…å®¹ï¼Œå¡«å†™è¯¥å¯¹è±¡ç‰©æœ€ç»ˆä½¿ç”¨çš„ç”¨é€”é¢†åŸŸï¼ˆä¾‹å¦‚ï¼šç‡ƒæ–™ç”µæ± æ±½è½¦ã€æ°¢èƒ½åŸºç¡€è®¾æ–½ï¼‰ã€‚",
                "æƒåˆ©è¦æ±‚çš„å¯¹è±¡": "æ ¹æ®æ–‡ç« å†…å®¹ï¼Œç”¨ä¸€ä¸ªè¯æè¿°æƒåˆ©è¦æ±‚å¯¹è±¡ã€‚å¦‚æœ‰å¤šä¸ªè¯·å…¨éƒ¨å¡«å†™ï¼ˆä¾‹å¦‚ï¼šç½ä½“;åˆ¶é€ æ–¹æ³•ï¼‰",
                "æŠ€æœ¯å¼€å‘èƒŒæ™¯ä¸è¯¾é¢˜": "è¯·åŸºäºæ–‡æ¡£å…·ä½“æè¿°èƒŒæ™¯ä¸æ‰€é¢ä¸´çš„è¯¾é¢˜",
                "æŠ€æœ¯å¼€å‘è¯¾é¢˜åˆ†ç±»": "å°†è¦è§£å†³çš„æŠ€æœ¯è¯¾é¢˜ç”¨ä¸€ä¸ªè¯è¡¨è¾¾ï¼ˆä¾‹å¦‚ï¼šå¼ºåº¦æå‡ã€åˆšæ€§æå‡ï¼‰ï¼Œå¯å¡«å†™å¤šä¸ªã€‚ï¼ˆå¼ºåº¦æå‡;åˆšæ€§æå‡ï¼‰",
                "è§£å†³æ‰‹æ®µæ¦‚è¦": "è¯·å…·ä½“æè¿°ä¸ºäº†è§£å†³è¯¾é¢˜æ‰€é‡‡å–çš„æ‰‹æ®µï¼ˆåŸºäºæ–‡ç« å†…å®¹ï¼‰ã€‚",
                "è§£å†³æ‰‹æ®µåˆ†ç±»": "è¯·ç”¨ä¸€ä¸ªè¯è¡¨è¾¾è§£å†³æ‰‹æ®µï¼ˆä¾‹å¦‚ï¼šææ–™æ”¹è‰¯ã€å½¢çŠ¶æ”¹è‰¯ã€å¸ƒå±€æ”¹è‰¯ã€è¡¨é¢å¤„ç†ï¼‰ï¼Œå¯å¡«å†™å¤šä¸ªã€‚ï¼ˆå¸ƒå±€æ”¹è‰¯;è¡¨é¢å¤„ç†ï¼‰",
                "å›½é™…ä¸“åˆ©åˆ†ç±»": "å¦‚æœ‰å›½é™…ä¸“åˆ©åˆ†ç±»ï¼ˆä¾‹å¦‚ï¼šF17C 1/06ï¼‰ï¼Œè¯·å‡†ç¡®å®Œæ•´å¡«å†™ã€‚å¯å¡«å†™å¤šä¸ªï¼ˆF16J 12/00;F17C 13/00ï¼‰ã€‚å¦‚æ— åˆ™ä¸ºç©ºã€‚è¯·è½¬æ¢ä¸ºåŠè§’å­—ç¬¦åå¡«å†™ã€‚",
                "ç°æœ‰æŠ€æœ¯æ–‡çŒ®": "è¯·å¡«å†™å…ˆè¡ŒæŠ€æœ¯æ–‡çŒ®çš„è¯†åˆ«ç¼–å·ï¼Œå¦‚ä¸“åˆ©å…¬å¼€ç¼–å·ï¼ˆä¾‹å¦‚ï¼šç‰¹å¼€2021-156312å·ï¼‰ã€‚å¦‚æ— åˆ™ä¸ºç©ºã€‚å¯å¡«å†™å¤šä¸ªã€‚ï¼ˆç‰¹å¼€2021-156312å·;ç‰¹å¼€2021-113560å·ï¼‰",
                "Fé¡¹åˆ†ç±»": "å¦‚æœ‰Fé¡¹åˆ†ç±»ï¼Œè¯·å‡†ç¡®å¡«å†™ã€‚å¦‚æ— åˆ™ä¸ºç©ºã€‚"
            }}

            ## ä¸“åˆ©å…¨æ–‡:
            {text}
            """)
            
            korean_prompt = ChatPromptTemplate.from_template("""
            ë‹¤ìŒì€ íŠ¹í—ˆ ì „ë¬¸ì…ë‹ˆë‹¤. ì§€ì •ëœ í•­ëª©ì— ë”°ë¼ ì •í™•í•œ JSON í˜•ì‹(Python ë”•ì…”ë„ˆë¦¬ í˜•ì‹)ìœ¼ë¡œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

            âš ï¸ ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš” (ë¬¸ì¥ì´ë‚˜ ì£¼ì„ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”).
            âš ï¸ ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œì— ë”°ë¼, ê° í•­ëª©ì€ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

            ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:
            {{
                "ë°œëª…ì˜ ëª…ì¹­": "...",
                "ì¶œì›ì¸": "ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶œì›ì¸ì„ ê¸°ì¬í•´ì£¼ì„¸ìš”. ë³µìˆ˜ í‘œê¸° ê°€ëŠ¥ (ì˜ˆ: ë„ìš”íƒ€ìë™ì°¨ì£¼ì‹íšŒì‚¬;ë„ìš”íƒ€ìë™ì§ê¸°ì£¼ì‹íšŒì‚¬)",
                "ë°œëª…ì": "ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°œëª…ìë¥¼ ê¸°ì¬í•´ì£¼ì„¸ìš”. ë³µìˆ˜ í‘œê¸° ê°€ëŠ¥ (ì˜ˆ: íŠ¹í—ˆ íƒ€ë¡œ;íŠ¹í—ˆ í•˜ë‚˜ì½”)",
                "ê³µê°œë²ˆí˜¸": "...",
                "ê³µê°œì¼": "ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³µê°œì¼ì„ ê¸°ì¬í•´ì£¼ì„¸ìš”. ì„œê¸°ë¡œ ë³€í™˜í•˜ê³ , ì „ë¶€ ë°˜ê° ë¬¸ìë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. (ì˜ˆ: 2025/2/6)",
                "ìš”ì•½": "ê³¼ì œ, íŠ¹í—ˆ ì²­êµ¬ ë²”ìœ„, í•´ê²°í•˜ë ¤ëŠ” ë¬¸ì œ, ë°œëª…ì˜ íš¨ê³¼ì™€ ê´€ë ¨ëœ ë‚´ìš©ì„ 300ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.",
                "ëŒ€ìƒë¬¼": "ë°œëª…ì˜ ëª…ì¹­ì—ì„œ íŠ¹í—ˆì˜ ëŒ€ìƒë¬¼ì„ í•œ ë‹¨ì–´ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš”. ë³µìˆ˜ í‘œê¸° ê°€ëŠ¥ (ì˜ˆ: íƒ±í¬;ë…¸ì¦)",
                "ëŒ€ìƒë¬¼ì˜ ìµœì¢… ìš©ë„": "ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ìƒë¬¼ì´ ì‚¬ìš©ë˜ëŠ” ìµœì¢… ìš©ë„ ë¶„ì•¼ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš” (ì˜ˆ: ìˆ˜ì†Œ ì¸í”„ë¼, ì—°ë£Œ ì „ì§€ ìë™ì°¨).",
                "ì²­êµ¬í•­ ëŒ€ìƒ": "ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì²­êµ¬í•­ì˜ ëŒ€ìƒì„ í•œ ë‹¨ì–´ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš”. ë³µìˆ˜ í•­ëª©ì´ ìˆëŠ” ê²½ìš° ëª¨ë‘ ëª…ì‹œí•´ì£¼ì„¸ìš” (ì˜ˆ: íƒ±í¬;ì œì¡° ë°©ë²•).",
                "ê¸°ìˆ  ê°œë°œ ë°°ê²½ ë° ê³¼ì œ": "ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°°ê²½ ë° ê³¼ì œë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.",
                "ê¸°ìˆ  ê°œë°œ ê³¼ì œ ë¶„ë¥˜": "í•´ê²°í•˜ë ¤ëŠ” ê¸°ìˆ  ê³¼ì œë¥¼ í•œ ë‹¨ì–´ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš” (ì˜ˆ: ê°•ë„ í–¥ìƒ;ê°•ì„± í–¥ìƒ). ë³µìˆ˜ í‘œê¸° ê°€ëŠ¥.",
                "í•´ê²° ìˆ˜ë‹¨ ê°œìš”": "ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì œ í•´ê²° ìˆ˜ë‹¨ì„ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.",
                "í•´ê²° ìˆ˜ë‹¨ ë¶„ë¥˜": "í•´ê²° ìˆ˜ë‹¨ì„ í•œ ë‹¨ì–´ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš” (ì˜ˆ: ì¬ë£Œ ê°œì„ , í˜•ìƒ ê°œì„ , ë°°ì¹˜ ê°œì„ , í‘œë©´ ê°€ê³µ). ë³µìˆ˜ í‘œê¸° ê°€ëŠ¥.",
                "êµ­ì œ íŠ¹í—ˆ ë¶„ë¥˜": "êµ­ì œ íŠ¹í—ˆ ë¶„ë¥˜(ì˜ˆ: F17C 1/06)ê°€ ìˆìœ¼ë©´ ì •í™•í•˜ê²Œ ëª¨ë‘ ê¸°ì¬í•´ì£¼ì„¸ìš”. ë³µìˆ˜ í‘œê¸° ê°€ëŠ¥. ì—†ìœ¼ë©´ ê³µë°±. ì „ë¶€ ë°˜ê° ë¬¸ìë¡œ ê¸°ì…í•´ì£¼ì„¸ìš”.",
                "ì„ í–‰ ê¸°ìˆ  ë¬¸í—Œ": "ì„ í–‰ ê¸°ìˆ  ë¬¸í—Œì˜ ì‹ë³„ ì½”ë“œ(ì˜ˆ: ê³µê°œíŠ¹í—ˆë²ˆí˜¸ ë“±)ë¥¼ ê¸°ì¬í•´ì£¼ì„¸ìš”. ì—†ìœ¼ë©´ ê³µë°±. ë³µìˆ˜ í‘œê¸° ê°€ëŠ¥.",
                "F-terms": "F-termsê°€ ìˆìœ¼ë©´ ì •í™•íˆ, ì—†ìœ¼ë©´ ê³µë°±."
            }}

            ## íŠ¹í—ˆ ì „ë¬¸:
            {text}
            """)
            
            english_prompt = ChatPromptTemplate.from_template("""
            Please extract the specified items from the following patent document in accurate JSON format (Python dictionary format).

            âš ï¸ Only return the JSON format (do not add any explanatory text or comments).
            âš ï¸ Follow the output format example, and write each item concretely based on the content of the document.

            Output format example:
            {{
                "Title of Invention": "...",
                "Applicant": "State the applicant based on the document. Multiple entries allowed (e.g., Toyota Motor Corporation;Toyota Industries Corporation)",
                "Inventor": "State the inventor(s) based on the document. Multiple entries allowed (e.g., Tokkyo Taro;Tokkyo Hanako)",
                "Publication Number": "...",
                "Publication Date": "State the publication date based on the document. Convert to Western calendar and use half-width characters (e.g., 2025/2/6)",
                "Abstract": "Summarize the challenges, claims, problems the invention addresses, and effects of the invention in less than 300 characters.",
                "Subject Matter": "Describe the subject of the invention in one word (e.g., Tank). Multiple entries allowed (e.g., Tank;Nozzle).",
                "Final Use of the Subject": "Based on the document, describe the final application field of the subject (e.g., fuel cell vehicle, hydrogen infrastructure).",
                "Claim Target": "Describe the object of the claims in one word. If there are multiple, specify all (e.g., Tank;Manufacturing Method).",
                "Background and Issues": "Clearly describe the background and technical issues based on the document.",
                "Technical Issue Classification": "Summarize the technical issue to be solved in one word (e.g., Strength Enhancement;Rigidity Enhancement). Multiple entries allowed.",
                "Summary of Solution": "Describe in detail the means taken to solve the problem (based on the document).",
                "Solution Classification": "Express the type of solution in one word (e.g., Material Improvement, Shape Optimization, Layout Modification, Surface Treatment). Multiple entries allowed.",
                "International Patent Classification": "If available, write all International Patent Classifications accurately (e.g., F17C 1/06). Use half-width characters. Leave blank if not applicable.",
                "Prior Art Documents": "Include identifiers for prior art (e.g., publication numbers such as JP2021-156312). Leave blank if not applicable. Multiple entries allowed.",
                "F-Term": "State F-terms if available. If not, leave blank."
            }}

            ## Patent Full Text:
            {text}
            """)

            è£œå®Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ = ChatPromptTemplate.from_template("""
            ä»¥ä¸‹ã®ç‰¹è¨±å…¨æ–‡ã®å†…ã€ã€ç™ºæ˜ã®è©³ç´°ãªèª¬æ˜ã€‘ã€ã€èª²é¡Œã€‘ã€ã€ç™ºæ˜ã®åŠ¹æœã€‘ã€ã€ç™ºæ˜ã‚’å®Ÿæ–½ã™ã‚‹ãŸã‚ã®å½¢æ…‹ã€‘ã€ã€å®Ÿæ–½ä¾‹ã€‘ã€ã€å…ˆè¡ŒæŠ€è¡“æ–‡çŒ®ã€‘ã‚’ã‚‚ã¨ã«ã€"{field}" ã®å€¤ã‚’æ­£ç¢ºã«æ—¥æœ¬èªã¾ãŸã¯æ•°å­—ãƒ»ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
            âš ï¸ è§£èª¬ã‚„ã‚³ãƒ¡ãƒ³ãƒˆã¯ä¸è¦ã§ã™ã€‚è©²å½“ã™ã‚‹å†…å®¹ãŒãªã„å ´åˆã¯ç©ºæ–‡å­—ï¼ˆ""ï¼‰ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚

            ## ç‰¹è¨±å…¨æ–‡:
            {text}
            """)
            
            def get_prompt_for_language(lang_code):
                if lang_code == "ja":
                    return japanese_prompt
                elif lang_code.startswith("zh"):
                    return chinese_prompt
                elif lang_code == "en":
                    return english_prompt
                elif lang_code == "ko":
                    return korean_prompt
                else:
                    return japanese_prompt  # fallback
                
            è£œå®Œ_chain = LLMChain(llm=llm, prompt=è£œå®Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ)
            
            def normalize_keys(record, lang_code):
                key_map = {
                    "ja": {},  # æ—¥æœ¬èªãªã‚‰ãã®ã¾ã¾
                    "zh": {
                        "å‘æ˜åç§°": "ç™ºæ˜ã®åç§°",
                        "ç”³è¯·äºº": "å‡ºé¡˜äºº",
                        "å‘æ˜äºº": "ç™ºæ˜è€…",
                        "å…¬å¼€ç¼–å·": "å…¬é–‹ç•ªå·",
                        "å…¬å¼€æ—¥": "å…¬é–‹æ—¥",
                        "æ‘˜è¦": "è¦ç´„",
                        "å¯¹è±¡ç‰©": "å¯¾è±¡ç‰©",
                        "å¯¹è±¡ç‰©çš„æœ€ç»ˆç”¨é€”": "å¯¾è±¡ç‰©ã®æœ€çµ‚ç”¨é€”",
                        "æƒåˆ©è¦æ±‚çš„å¯¹è±¡": "è«‹æ±‚é …ã®å¯¾è±¡",
                        "æŠ€æœ¯å¼€å‘èƒŒæ™¯ä¸è¯¾é¢˜": "æŠ€è¡“é–‹ç™ºã®èƒŒæ™¯ãƒ»èª²é¡Œ",
                        "æŠ€æœ¯å¼€å‘è¯¾é¢˜åˆ†ç±»": "æŠ€è¡“é–‹ç™ºèª²é¡Œåˆ†é¡",
                        "è§£å†³æ‰‹æ®µæ¦‚è¦": "è§£æ±ºæ‰‹æ®µã®æ¦‚è¦",
                        "è§£å†³æ‰‹æ®µåˆ†ç±»": "è§£æ±ºæ‰‹æ®µåˆ†é¡",
                        "å›½é™…ä¸“åˆ©åˆ†ç±»": "å›½éš›ç‰¹è¨±åˆ†é¡",
                        "å…ˆè¡ŒæŠ€æœ¯æ–‡çŒ®": "å…ˆè¡ŒæŠ€è¡“æ–‡çŒ®",
                        "Fé¡¹åˆ†ç±»": "Fã‚¿ãƒ¼ãƒ ",
                    },
                    "ko": {
                        "ë°œëª…ì˜ ëª…ì¹­": "ç™ºæ˜ã®åç§°",
                        "ì¶œì›ì¸": "å‡ºé¡˜äºº",
                        "ë°œëª…ì": "ç™ºæ˜è€…",
                        "ê³µê°œë²ˆí˜¸": "å…¬é–‹ç•ªå·",
                        "ê³µê°œì¼": "å…¬é–‹æ—¥",
                        "ìš”ì•½": "è¦ç´„",
                        "ëŒ€ìƒë¬¼": "å¯¾è±¡ç‰©",
                        "ëŒ€ìƒë¬¼ì˜ ìµœì¢… ìš©ë„": "å¯¾è±¡ç‰©ã®æœ€çµ‚ç”¨é€”",
                        "ì²­êµ¬í•­ ëŒ€ìƒ": "è«‹æ±‚é …ã®å¯¾è±¡",
                        "ê¸°ìˆ  ê°œë°œ ë°°ê²½ ë° ê³¼ì œ": "æŠ€è¡“é–‹ç™ºã®èƒŒæ™¯ãƒ»èª²é¡Œ",
                        "ê¸°ìˆ  ê°œë°œ ê³¼ì œ ë¶„ë¥˜": "æŠ€è¡“é–‹ç™ºèª²é¡Œåˆ†é¡",
                        "í•´ê²° ìˆ˜ë‹¨ ê°œìš”": "è§£æ±ºæ‰‹æ®µã®æ¦‚è¦",
                        "í•´ê²° ìˆ˜ë‹¨ ë¶„ë¥˜": "è§£æ±ºæ‰‹æ®µåˆ†é¡",
                        "êµ­ì œ íŠ¹í—ˆ ë¶„ë¥˜": "å›½éš›ç‰¹è¨±åˆ†é¡",
                        "ì„ í–‰ ê¸°ìˆ  ë¬¸í—Œ": "å…ˆè¡ŒæŠ€è¡“æ–‡çŒ®",
                        "F-terms": "Fã‚¿ãƒ¼ãƒ ",
                    },
                    "en": {
                        "Title of Invention": "ç™ºæ˜ã®åç§°",
                        "Applicant": "å‡ºé¡˜äºº",
                        "Inventor": "ç™ºæ˜è€…",
                        "Publication Number": "å…¬é–‹ç•ªå·",
                        "Publication Date": "å…¬é–‹æ—¥",
                        "Abstract": "è¦ç´„",
                        "Subject Matter": "å¯¾è±¡ç‰©",
                        "Final Use of the Subject": "å¯¾è±¡ç‰©ã®æœ€çµ‚ç”¨é€”",
                        "Claim Target": "è«‹æ±‚é …ã®å¯¾è±¡",
                        "Background and Issues": "æŠ€è¡“é–‹ç™ºã®èƒŒæ™¯ãƒ»èª²é¡Œ",
                        "Technical Issue Classification": "æŠ€è¡“é–‹ç™ºèª²é¡Œåˆ†é¡",
                        "Summary of Solution": "è§£æ±ºæ‰‹æ®µã®æ¦‚è¦",
                        "Solution Classification": "è§£æ±ºæ‰‹æ®µåˆ†é¡",
                        "International Patent Classification": "å›½éš›ç‰¹è¨±åˆ†é¡",
                        "Prior Art Documents": "å…ˆè¡ŒæŠ€è¡“æ–‡çŒ®",
                        "F-Term": "Fã‚¿ãƒ¼ãƒ ",
                    }
                }

                # ãƒãƒƒãƒ”ãƒ³ã‚°é©ç”¨
                lang_base = lang_code.split("-")[0]  # zh-cn â†’ zh ãªã©ã«å¯¾å¿œ
                mapping = key_map.get(lang_base, {})
                normalized = {}
                for k, v in record.items():
                    new_key = mapping.get(k.strip(), k.strip())  # å¯¾å¿œãŒãªã‘ã‚Œã°ãã®ã¾ã¾
                    normalized[new_key] = v
                return normalized


            # === ã‚»ã‚¯ã‚·ãƒ§ãƒ³æŠ½å‡ºé–¢æ•° ===
            def extract_sections(text, lang_code):
                if lang_code == "ja":     
                    patterns = {
                        "title": r"ã€ç™ºæ˜ã®åç§°ã€‘(.+?)\n",
                        "Publication_number": r"å…¬é–‹ç•ªå·(.+?)\n",
                        "Publication_date": r"å…¬é–‹æ—¥(.+?)\n",
                        "applicant": r"å‡ºé¡˜äºº([\s\S]+?)(?=ã€|$)",
                        "inventor": r"ç™ºæ˜è€…([\s\S]+?)(?=ã€|$)",
                        "IPC": r"å›½éš›ç‰¹è¨±åˆ†é¡([\s\S]+?)(?=ã€|$)",
                        "abstract": r"ã€èª²é¡Œã€‘([\s\S]+?)(?=ã€|$)",
                        "claims": r"ã€ç‰¹è¨±è«‹æ±‚ã®ç¯„å›²ã€‘([\s\S]+?)(?=ã€|$)",
                        "description": r"ã€ç™ºæ˜ã®è©³ç´°ãªèª¬æ˜ã€‘([\s\S]+?)(?=ã€|$)",
                        "technical_problem": r"ã€ç™ºæ˜ãŒè§£æ±ºã—ã‚ˆã†ã¨ã™ã‚‹èª²é¡Œã€‘([\s\S]+?)(?=ã€|$)",
                        "impact": r"ã€ç™ºæ˜ã®åŠ¹æœã€‘([\s\S]+?)(?=ã€|$)",
                        "prior_art": r"ã€å…ˆè¡ŒæŠ€è¡“æ–‡çŒ®([\s\S]+?)(?=ç™º|$)",
                        "detail_description": r"ã€ç™ºæ˜ã‚’å®Ÿæ–½ã™ã‚‹ãŸã‚ã®å½¢æ…‹ã€‘([\s\S]+?)(?=ã€|$)",
                        "Example": r"ã€å®Ÿæ–½ä¾‹ã€‘([\s\S]+?)(?=ã€|$)",
                        "F_code": r"Fã‚¿ãƒ¼ãƒ ([\s\S]+?)(?=ã€|$)",
                    }
                    
                    
                elif lang_code.startswith("zh"):
                    patterns = {
                        "title": r"å‘æ˜åç§°(.+?)\n",
                        "Publication_number": r"ç”³è¯·å…¬å¸ƒå· / å…¬å¼€å·(.+?)\n",
                        "Publication_date": r"ç”³è¯·å…¬å¸ƒæ—¥(.+?)\n",
                        "applicant": r"ç”³è¯·äºº([\s\S]+?)(?=ã€|$)",
                        "inventor": r"å‘æ˜äºº([\s\S]+?)(?=ã€|$)",
                        "IPC": r"å›½é™…ä¸“åˆ©åˆ†ç±» (Int.Cl.)([\s\S]+?)(?=ã€|$)",
                        "abstract": r"èƒŒæ™¯æŠ€æœ¯([\s\S]+?)(?=ã€|$)",
                        "claims": r"æƒåˆ©è¦æ±‚ä¹¦([\s\S]+?)(?=ã€|$)",
                        "description": r"è¯´æ˜ä¹¦([\s\S]+?)(?=ã€|$)",
                        "technical_problem": r"å…·ä½“åœ¨èƒŒæ™¯æŠ€æœ¯ä¸­è¡¨è¿°([\s\S]+?)(?=ã€|$)",
                        "impact": r"å‘æ˜å†…å®¹å†…([\s\S]+?)(?=ã€|$)",
                        "prior_art": r"å…ˆè¡ŒæŠ€æœ¯æ–‡çŒ®([\s\S]+?)(?=ç™º|$)",
                        "detail_description": r"å…·ä½“å®æ–½æ–¹å¼([\s\S]+?)(?=ã€|$)",
                        "Example": r"å®æ–½ä¾‹([\s\S]+?)(?=ã€|$)",
                        "F_code": r"Fã‚¿ãƒ¼ãƒ ([\s\S]+?)(?=ã€|$)",
                    }
                    
                elif lang_code == "en":
                    patterns = {
                        "title": r"Title(.+?)\n",
                        "Publication_number": r"Publication Number(.+?)\n",
                        "Publication_date": r"Publication Date(.+?)\n",
                        "applicant": r"Applicant([\s\S]+?)(?=ã€|$)",
                        "inventor": r"Inventor(s)([\s\S]+?)(?=ã€|$)",
                        "IPC": r"International Class / Int. Cl.([\s\S]+?)(?=ã€|$)",
                        "abstract": r"Background of the Invention([\s\S]+?)(?=ã€|$)",
                        "claims": r"Claims([\s\S]+?)(?=ã€|$)",
                        "description": r"Detailed Description([\s\S]+?)(?=ã€|$)",
                        "technical_problem": r"	Background([\s\S]+?)(?=ã€|$)",
                        "impact": r"Background([\s\S]+?)(?=ã€|$)",
                        "prior_art": r"References Cited / Prior Art([\s\S]+?)(?=ç™º|$)",
                        "detail_description": r"Detailed Description ([\s\S]+?)(?=ã€|$)",
                        "Example": r"Detailed Description([\s\S]+?)(?=ã€|$)",
                        "F_code": r"Fã‚¿ãƒ¼ãƒ ([\s\S]+?)(?=ã€|$)",
                    }
                
                elif lang_code == "ko":
                   patterns = {
                        "title": r"ë°œëª…ì˜\s*ëª…ì¹­[:ï¼š]?\s*(.+)",  # ã€Œ:ã€ã¾ãŸã¯ã€Œï¼šã€ã‚ã‚Šãƒ»ãªã—ä¸¡å¯¾å¿œ
                        "Publication_number": r"ê³µê°œë²ˆí˜¸[:ï¼š]?\s*(.+)",
                        "Publication_date": r"ê³µê°œì¼[:ï¼š]?\s*(.+)",
                        "applicant": r"ì¶œì›ì¸[:ï¼š]?\s*([\s\S]+?)(?:\n\n|$)",  # æ¬¡ã®ç©ºè¡Œã¾ãŸã¯æ–‡æœ«ã¾ã§
                        "inventor": r"ë°œëª…ì[:ï¼š]?\s*([\s\S]+?)(?:\n\n|$)",
                        "IPC": r"êµ­ì œíŠ¹í—ˆë¶„ë¥˜[:ï¼š]?\s*([\s\S]+?)(?:\n\n|$)",
                        "abstract": r"ë°°ê²½ê¸°ìˆ [:ï¼š]?\s*([\s\S]+?)(?:\n\n|$)",
                        "claims": r"(?:ì²­êµ¬í•­|ì²­êµ¬ë²”ìœ„)[:ï¼š]?\s*([\s\S]+?)(?:\n\n|$)",
                        "description": r"ìƒì„¸í•œ\s*ì„¤ëª…[:ï¼š]?\s*([\s\S]+?)(?:\n\n|$)",
                        "technical_problem": r"ë°œëª…ì˜\s*ëª©ì [:ï¼š]?\s*([\s\S]+?)(?:\n\n|$)",
                        "impact": r"ë°œëª…ì˜\s*íš¨ê³¼[:ï¼š]?\s*([\s\S]+?)(?:\n\n|$)",
                        "prior_art": r"ì„ í–‰\s*ê¸°ìˆ \s*ë¬¸í—Œ[:ï¼š]?\s*([\s\S]+?)(?:\n\n|$)",
                        "detail_description": r"ë°œëª…ì„\s*ì‹¤ì‹œí•˜ê¸°\s*ìœ„í•œ\s*í˜•íƒœ[:ï¼š]?\s*([\s\S]+?)(?:\n\n|$)",
                        "Example": r"ì‹¤ì‹œì˜ˆ[:ï¼š]?\s*([\s\S]+?)(?:\n\n|$)",
                        "F_code": r"F-?íƒ€?ë¦„[:ï¼š]?\s*([\s\S]+?)(?:\n\n|$)",  # Fã‚¿ãƒ¼ãƒ  (éŸ“å›½ç‰¹è¨±ã§ã¯åŸºæœ¬å­˜åœ¨ã—ãªã„ãŒä¿é™ºã§)
                    }
                    
                sections = {}
                for key, pattern in patterns.items():
                    match = re.search(pattern, text)
                    sections[key] = match.group(1).strip() if match else ""
                return sections
                    

            # === æ¬ æè£œå®Œé–¢æ•° ===
            def get_target_fields(lang_code):
                base_fields = [
                    "ç™ºæ˜ã®åç§°", "å‡ºé¡˜äºº", "ç™ºæ˜è€…", "å…¬é–‹ç•ªå·", "å…¬é–‹æ—¥",
                    "è¦ç´„", "å¯¾è±¡ç‰©", "å¯¾è±¡ç‰©ã®æœ€çµ‚ç”¨é€”", "è«‹æ±‚é …ã®å¯¾è±¡",
                    "æŠ€è¡“é–‹ç™ºã®èƒŒæ™¯ãƒ»èª²é¡Œ", "æŠ€è¡“é–‹ç™ºèª²é¡Œåˆ†é¡", "è§£æ±ºæ‰‹æ®µã®æ¦‚è¦", "è§£æ±ºæ‰‹æ®µåˆ†é¡",
                    "å›½éš›ç‰¹è¨±åˆ†é¡", "å…ˆè¡ŒæŠ€è¡“æ–‡çŒ®"
                ]
                if lang_code == "ja":
                    base_fields.append("Fã‚¿ãƒ¼ãƒ ")
                return base_fields
            
            
            
            
            def fill_missing_fields(record, full_text):
                for field in target_fields:
                    value = record.get(field)
                    if (
                        value is None or
                        (isinstance(value, str) and not value.strip()) or
                        (isinstance(value, list) and not any(value))
                    ):
                        print(f"ğŸ”„ æ¬ æè£œå®Œä¸­: {field}")
                        try:
                            filled = è£œå®Œ_chain.run({
                                "field": field,
                                "existing_values": value,
                                "full_text": full_text
                            })
                            record[field] = filled
                        except Exception as e:
                            print(f"âš ï¸ {field} è£œå®Œã‚¨ãƒ©ãƒ¼: {e}")
                return record


            # === å®Ÿè¡Œå‡¦ç† ===
            filename = os.path.basename(docx_path)
            print(f"\nğŸ“˜ å‡¦ç†ä¸­: {filename}")

            try:
                doc = Document(docx_path)
                full_text = "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
                try:
                    lang_code = detect(full_text)
                except:
                    lang_code = "en"
                
                main_prompt = get_prompt_for_language(lang_code)
                target_fields = get_target_fields(lang_code)
                main_chain = LLMChain(llm=llm, prompt=main_prompt)
                sections = extract_sections(full_text, lang_code)
                #merged_text = "\n".join([f"ã€{k}ã€‘\n{v}" for k, v in sections.items() if v])
                # â¬‡ï¸ OCRã‹ã‚‰ã®è¿½åŠ ãƒ†ã‚­ã‚¹ãƒˆã‚’ merged_text ã«è£œå®Œ
                ocr_text = extract_text_from_docx_images(docx_path)
                merged_text = full_text + "\n\n" + ocr_text  # OCRã‚’å¾Œã‚ã«è¿½åŠ 
                if ocr_text:
                    merged_text += f"\n\nã€OCRæŠ½å‡ºã€‘\n{ocr_text}"

                result = main_chain.run({"text": merged_text})
                if not result.strip().startswith("{"):
                    raise ValueError("JSONå½¢å¼ã§ãªã„å¿œç­”")

                parsed = json.loads(result)
                parsed["ãƒ•ã‚¡ã‚¤ãƒ«å"] = filename
                
                parsed = normalize_keys(parsed, lang_code)

            except Exception as e:
                print(f"âš ï¸ ãƒ¡ã‚¤ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                parsed = {
                    "ãƒ•ã‚¡ã‚¤ãƒ«å": filename,
                    "æŠ½å‡ºã‚¨ãƒ©ãƒ¼": str(e)
                }
                merged_text = full_text  # ã‚¨ãƒ©ãƒ¼æ™‚ã«ã‚‚è£œå®Œã§ãã‚‹ã‚ˆã†

            completed = fill_missing_fields(parsed, merged_text)
            time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            return completed


        def save_to_excel(data_list, output_dir="outputs"):
            os.makedirs(output_dir, exist_ok=True)
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"ç‰¹è¨±_æŠ½å‡ºçµæœ_{timestamp}_{len(data_list)}ä»¶.xlsx"
            # å…¨ãƒ¬ã‚³ãƒ¼ãƒ‰ã«å¯¾ã—ã¦æ—¥æœ¬èªç¿»è¨³ã‚’é©ç”¨
            # âœ… ç¿»è¨³å¯¾è±¡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            fields_to_translate = [
                "ç™ºæ˜ã®åç§°", "è¦ç´„", "å¯¾è±¡ç‰©", "å¯¾è±¡ç‰©ã®æœ€çµ‚ç”¨é€”", "è«‹æ±‚é …ã®å¯¾è±¡",
                "æŠ€è¡“é–‹ç™ºã®èƒŒæ™¯ãƒ»èª²é¡Œ", "æŠ€è¡“é–‹ç™ºèª²é¡Œåˆ†é¡", "è§£æ±ºæ‰‹æ®µã®æ¦‚è¦", "è§£æ±ºæ‰‹æ®µåˆ†é¡"
            ]

            translated_data = []
            for record in data_list:
                translated_record = {}
                for k, v in record.items():
                    if isinstance(v, str) and k in fields_to_translate:
                        translated_record[k] = translate_to_japanese(v)  # ğŸ”¥ ã“ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã ã‘ç¿»è¨³
                    else:
                        translated_record[k] = v  # ãã®ä»–ã¯ãã®ã¾ã¾
                translated_data.append(translated_record)

            # ä¿å­˜
            df = pd.DataFrame(translated_data)
            df.to_excel(os.path.join(output_dir, filename), index=False)
            return filename
        
        def main_app():
            if 'processed_data' not in st.session_state:
                st.session_state.processed_data = None
            if 'filename' not in st.session_state:
                st.session_state.filename = None

            uploaded_files = st.file_uploader("ç‰¹è¨±PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°å¯ï¼‰", type="pdf", accept_multiple_files=True)
            if uploaded_files and st.button("å‡¦ç†ã‚’é–‹å§‹"):
                status_area = st.empty()
                progress = st.progress(0)
                processed_data = []
                
                for i, file in enumerate(uploaded_files):
                    with open(file.name, 'wb') as f:
                        f.write(file.getbuffer())
                    status_area.info(f"{file.name} ã‚’Google Driveã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")

                    pdf_id, saved_name = upload_to_drive(folder_id=PDF_input_folder, file_path=file.name)
                    progress.progress((i + 1) / len(uploaded_files) * 0.2)
                    
                    status_area.info(f"{file.name} ã‚’Google Docsã«å¤‰æ›ä¸­...")
                    doc_id = convert_pdf_to_doc(pdf_id, output_folder_id=DOCS_output_folder)
                    progress.progress((i + 1) / len(uploaded_files) * 0.4)

                    docx_path = f"temp_{uuid.uuid4().hex[:8]}.docx"
                    download_doc_as_docx(doc_id, docx_path)

                    status_area.info(f"{file.name} ã®å†…å®¹ã‚’æ§‹é€ åŒ–ä¸­...")
                    structured = process_docx_file(docx_path)
                    processed_data.append(structured)
                    progress.progress((i + 1) / len(uploaded_files) * 0.8)

                    os.remove(docx_path)  # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤

                    status_area.info(f"{file.name} ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ä¸­...")
                    delete_from_drive(pdf_id)
                    delete_from_drive(doc_id)

                status_area.success("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ã‚¨ã‚¯ã‚»ãƒ«ã‚’ç”Ÿæˆä¸­ã€‚ã‚‚ã†å°‘ã—ã—ãŸã‚‰ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ãŒç¾ã‚Œã¾ã™ã€‚")
                filename = save_to_excel(processed_data)
                st.session_state.processed_data = processed_data
                st.session_state.filename = filename

                progress.progress(1.0)
            
            # 1. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ç”¨ã®ãƒ•ãƒ©ã‚°ã‚’åˆæœŸåŒ–
            if "downloaded" not in st.session_state:
                st.session_state.downloaded = False

            # 2. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã®å‡¦ç†
       
            if st.session_state.processed_data and st.session_state.filename:
                with open(f"outputs/{st.session_state.filename}", "rb") as f:
                    if st.download_button("çµæœExcelã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", f, file_name=st.session_state.filename):
                        st.success("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚")

                        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å‰Šé™¤
                        del st.session_state["processed_data"]
                        del st.session_state["filename"]

                        # ç”»é¢ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ãŸã‚ã«ä¸€æ™‚çš„ãªç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’è¡¨ç¤º
                        st.empty()  # æ˜ç¤ºçš„ãªç©ºç™½ã§UIãƒªã‚»ãƒƒãƒˆä»£ç”¨

        if __name__ == "__main__":
            main_app()
        

    elif mode == "ãƒ‡ãƒ¼ã‚¿è§£æã‚·ã‚¹ãƒ†ãƒ ":
        st.title("ãƒ‡ãƒ¼ã‚¿è§£æã‚·ã‚¹ãƒ†ãƒ ")
        st.write("ç‰¹è¨±è¦ç´„ã‚·ã‚¹ãƒ†ãƒ ã§å‡ºåŠ›ã—ãŸã‚¨ã‚¯ã‚»ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨ã„ã¦ã€ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»å¯è¦–åŒ–ã‚’è¡Œã„ã¾ã™ï¼")

        #st.image("./fig/ãƒ‡ãƒ¼ã‚¿åˆ†æã‚·ã‚¹ãƒ†ãƒ 1.jpg", use_container_width=True)
        #st.image("./fig/ãƒ‡ãƒ¼ã‚¿åˆ†æã‚·ã‚¹ãƒ†ãƒ 2.jpg", use_container_width=True)
        #st.image("./fig/ãƒ‡ãƒ¼ã‚¿åˆ†æã‚·ã‚¹ãƒ†ãƒ 3.jpg", use_container_width=True)

        
        def st_rag_langgraph():
            st.write("CSVãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯xlsxãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            uploaded_file = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=["csv", "xlsx"])

            if uploaded_file is not None:
                try:
                    # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦é©åˆ‡ã«å‡¦ç†
                    if uploaded_file.name.endswith(".csv"):
                        df = pd.read_csv(uploaded_file, index_col=0, header=0)
                
                    elif uploaded_file.name.endswith(".xlsx"):
                        df = pd.read_excel(uploaded_file, index_col=0, header=0)
                        
                    df_columns = df.columns#df_columnsã«å¤‰æ•°åã‚’æ ¼ç´
                    
                    #å‡ºé¡˜æ—¥ã®é¸æŠ
                    public_date = st.selectbox("å‡ºé¡˜æ—¥ã¾ãŸã¯å…¬é–‹æ—¥ã«é–¢é€£ã™ã‚‹é …ç›®ã‚’é¸æŠ", df_columns)
                    df[public_date] = pd.to_datetime(df[public_date], errors='coerce')
                    df["å‡ºé¡˜å¹´"] = df[public_date].dt.year
                    df["å‡ºé¡˜ä»¶æ•°"] = 1
                    
                    start_date = datetime(2000, 1, 1)
                    end_date = datetime(2025, 3, 31)
                    
                    term = st.slider("æœŸé–“",value=[start_date,end_date])
                    
                    _df = df[(df[public_date] >= term[0]) & (df[public_date] <= term[1])]
                    
                    st.write("ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:", _df.head())
                    
                    analyze_mode = st.sidebar.radio("åˆ†æãƒ¢ãƒ¼ãƒ‰é¸æŠ", ("ç‰¹è¨±å‡ºé¡˜ä»¶æ•°æŠŠæ¡", "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ","ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ—","ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆ"))
                
                    if analyze_mode == "ç‰¹è¨±å‡ºé¡˜ä»¶æ•°æŠŠæ¡":
                        applicants_list = st.selectbox("å‡ºé¡˜äººã«é–¢é€£ã™ã‚‹é …ç›®ã‚’é¸æŠã€‚", df_columns)
                        #ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚«ãƒ©ãƒ ã‚’é¸æŠè‚¢ã«ã™ã‚‹ã€‚è¤‡æ•°é¸æŠ
                        
                        df_clean = _df.dropna(subset=[applicants_list]).copy()
                        
                        # å‡ºé¡˜äººãƒªã‚¹ãƒˆåŒ–ï¼ˆã‚»ãƒŸã‚³ãƒ­ãƒ³åˆ†å‰²ï¼‰
                        df_clean["applicant_list"] = df_clean[applicants_list].apply(lambda x: [a.strip() for a in str(x).split(";")])

                        # ä¸€æ„ãªå‡ºé¡˜äººåä¸€è¦§ï¼ˆflattenã—ã¦ä¸€æ„ã«ï¼‰
                        flattened_applicants = [applicant for sublist in df_clean["applicant_list"] for applicant in sublist]
                        unique_values = sorted(pd.unique(flattened_applicants))

                        # è¡¨ç¤ºã™ã‚‹å‡ºé¡˜äººã‚’é¸æŠ
                        company = st.multiselect("è¡¨ç¤ºã™ã‚‹å‡ºé¡˜äºº", unique_values, default=unique_values )

                        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼šéƒ¨åˆ†ä¸€è‡´ï¼ˆcompanyã®ã„ãšã‚Œã‹ãŒå«ã¾ã‚Œã¦ã„ã‚‹ï¼‰
                        def company_filter(applicant_list):
                            return any(any(c in a for a in applicant_list) for c in company)

                        result = df_clean[df_clean["applicant_list"].apply(company_filter)]

                        # å…±åŒå‡ºé¡˜åˆ¤å®š
                        result["å…±åŒå‡ºé¡˜ã®æœ‰ç„¡(0:ç„¡,1:æœ‰)"] = result["applicant_list"].apply(lambda x: 1 if len(x) > 1 else 0)

                        # é›†è¨ˆã¨ãƒ—ãƒ­ãƒƒãƒˆ
                        summary = result.groupby(["å‡ºé¡˜å¹´", "å…±åŒå‡ºé¡˜ã®æœ‰ç„¡(0:ç„¡,1:æœ‰)"])["å‡ºé¡˜ä»¶æ•°"].sum().reset_index()
                        fig, ax = plt.subplots(figsize=(25,12))
                        sns.barplot(data=summary, x="å‡ºé¡˜å¹´", y="å‡ºé¡˜ä»¶æ•°", hue="å…±åŒå‡ºé¡˜ã®æœ‰ç„¡(0:ç„¡,1:æœ‰)", errorbar=None)
                        ax.set_xlabel("å‡ºé¡˜å¹´", fontsize=40)
                        ax.set_ylabel("å‡ºé¡˜ä»¶æ•°", fontsize=40)
                        ax.tick_params(axis='y', labelsize=15)
                        ax.tick_params(axis='x', labelsize=15)
                        # å‡¡ä¾‹ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’å¤‰æ›´
                        
                        legend = ax.legend(fontsize=25)  # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’12ã«è¨­å®š
                        legend.set_title("å…±åŒå‡ºé¡˜(ç„¡:0,æœ‰:1)", prop={'size': 25}) 

                        st.pyplot(fig)
                    
                    elif analyze_mode == "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ":
                                                
                        # æ¬ æå€¤ã‚’å‰Šé™¤
                        applicants_list = st.selectbox("å‡ºé¡˜äººã«é–¢é€£ã™ã‚‹é …ç›®ã‚’é¸æŠã€‚", df_columns)

                        # æ¬ æé™¤å»
                        df_clean = _df.dropna(subset=[applicants_list]).copy()
                        df_clean = df_clean.dropna(subset=["å¯¾è±¡ç‰©"]).copy()

                        # å¯¾è±¡ç‰©ã‚’ã‚»ãƒŸã‚³ãƒ­ãƒ³åˆ†å‰²ã—ã¦ãƒªã‚¹ãƒˆåŒ–
                        df_clean["object_list"] = df_clean["å¯¾è±¡ç‰©"].apply(lambda x: [a.strip() for a in str(x).split(";")])

                        # ä¸€æ„ãªå¯¾è±¡ç‰©ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—
                        flattened_object = [obj for sublist in df_clean["object_list"] for obj in sublist]
                        unique_values = sorted(pd.unique(flattened_object))

                        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é¸ã°ã›ã‚‹
                        company = st.multiselect("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å¯è¦–åŒ–ã™ã‚‹å¯¾è±¡ç‰©ã‚’é¸æŠã—ã¦ãã ã•ã„", unique_values, default=unique_values)

                        # éƒ¨åˆ†ä¸€è‡´ãƒ•ã‚£ãƒ«ã‚¿é–¢æ•°ï¼ˆå¯¾è±¡ç‰©ã«1ã¤ã§ã‚‚companyã®ã©ã‚Œã‹ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°OKï¼‰
                        def object_partial_match_filter(object_list):
                            return any(any(c in obj for obj in object_list) for c in company)

                        # ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
                        result = df_clean[df_clean["object_list"].apply(object_partial_match_filter)]
                        
                        
                        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®åˆ†å‰²å‡¦ç†
                        def split_keywords(text):
                            if pd.isna(text) or not isinstance(text, str):
                                return []
                            return [kw.strip() for kw in text.split(",") if kw.strip()]

                        result["èª²é¡Œãƒªã‚¹ãƒˆ"] = result["æŠ€è¡“é–‹ç™ºèª²é¡Œåˆ†é¡"].apply(split_keywords)
                        result["æŠ€è¡“ãƒªã‚¹ãƒˆ"] = result["è§£æ±ºæ‰‹æ®µåˆ†é¡"].apply(split_keywords)

                        # å…±èµ·ãƒšã‚¢ä½œæˆ
                        records = []
                        for row in result.itertuples():
                            for use in row.èª²é¡Œãƒªã‚¹ãƒˆ:
                                for tech in row.æŠ€è¡“ãƒªã‚¹ãƒˆ:
                                    records.append((use, tech))

                        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰
                        G = nx.Graph()
                        for use, tech in records:
                            G.add_node(use, type="èª²é¡Œ")
                            G.add_node(tech, type="æŠ€è¡“")
                            G.add_edge(use, tech)

                        # ãƒãƒ¼ãƒ‰ä½ç½®è¨ˆç®—
                        pos = nx.spring_layout(G, seed=42)

                        # Plotlyå‘ã‘ãƒ‡ãƒ¼ã‚¿å¤‰æ›
                        edge_x = []
                        edge_y = []
                        for edge in G.edges():
                            x0, y0 = pos[edge[0]]
                            x1, y1 = pos[edge[1]]
                            edge_x += [x0, x1, None]
                            edge_y += [y0, y1, None]

                        node_x = []
                        node_y = []
                        node_text = []
                        node_color = []
                        for node in G.nodes():
                            x, y = pos[node]
                            node_x.append(x)
                            node_y.append(y)
                            node_text.append(node)
                            node_color.append("skyblue" if G.nodes[node]['type'] == "èª²é¡Œ" else "lightgreen")

                        # Plotlyå›³ç”Ÿæˆ
                        edge_trace = go.Scatter(
                            x=edge_x, y=edge_y,
                            line=dict(width=0.5, color='#888'),
                            hoverinfo='none',
                            mode='lines')

                        node_trace = go.Scatter(
                            x=node_x, y=node_y,
                            mode='markers+text',
                            text=node_text,
                            textposition="top center",
                            hoverinfo='text',
                            marker=dict(
                                color=node_color,
                                size=12,
                                line_width=2))

                        fig_network = go.Figure(data=[edge_trace, node_trace],
                                    layout=go.Layout(
                                        title='èª²é¡Œãƒ»æŠ€è¡“ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³',
                                        showlegend=False,
                                        hovermode='closest',
                                        margin=dict(b=20,l=5,r=5,t=40),
                                        xaxis=dict(showgrid=False, zeroline=False),
                                        yaxis=dict(showgrid=False, zeroline=False))
                        )

                        # ======================
                        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆæŠ€è¡“è¦ç´ å´ã®ã¿ï¼‰
                        # ======================

                        # æŠ€è¡“ãƒªã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆTF-IDFï¼‰
                        tech_docs = [" ".join(techs) for techs in result["æŠ€è¡“ãƒªã‚¹ãƒˆ"]]
                        vectorizer = TfidfVectorizer()
                        X = vectorizer.fit_transform(tech_docs)

                        # KMeansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆä¾‹ï¼š5ã‚¯ãƒ©ã‚¹ã‚¿ï¼‰
                        kmeans = KMeans(n_clusters=5, random_state=42)
                        result["æŠ€è¡“ã‚¯ãƒ©ã‚¹ã‚¿"] = kmeans.fit_predict(X)

                        # ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®ä»£è¡¨èªï¼ˆé‡ã¿ä¸Šä½ï¼‰
                        terms = vectorizer.get_feature_names_out()
                        top_keywords_per_cluster = {}
                        order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]

                        for i in range(5):
                            top_keywords = [terms[ind] for ind in order_centroids[i, :5]]
                            top_keywords_per_cluster[i] = top_keywords

                        result["æŠ€è¡“ã‚¯ãƒ©ã‚¹ã‚¿_ä»£è¡¨èª"] = result["æŠ€è¡“ã‚¯ãƒ©ã‚¹ã‚¿"].map(top_keywords_per_cluster)

                        df_clustered = result[["ãƒ•ã‚¡ã‚¤ãƒ«å", "æŠ€è¡“ãƒªã‚¹ãƒˆ", "æŠ€è¡“ã‚¯ãƒ©ã‚¹ã‚¿", "æŠ€è¡“ã‚¯ãƒ©ã‚¹ã‚¿_ä»£è¡¨èª"]]
                        

                        #fig_network.show()
                        st.plotly_chart(fig_network, use_container_width=True)
                        st.write("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ:", df_clustered.head())
                        
                        #åˆ¥ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³
                        #result = _df.dropna(subset=[applicants_list]).copy()
                        
                        #result["èª²é¡Œãƒªã‚¹ãƒˆ"] = result["æŠ€è¡“é–‹ç™ºèª²é¡Œåˆ†é¡"].apply(split_keywords)
                        #result["æŠ€è¡“ãƒªã‚¹ãƒˆ"] = result["è§£æ±ºæ‰‹æ®µåˆ†é¡"].apply(split_keywords)
                        result["å‡ºé¡˜äººãƒªã‚¹ãƒˆ"] = result["å‡ºé¡˜äºº"].apply(split_keywords)
                        result["ç™ºæ˜è€…ãƒªã‚¹ãƒˆ"] = result["ç™ºæ˜è€…"].apply(split_keywords)

                        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰ï¼ˆå‡ºé¡˜äºº Ã— ç”¨é€”ï¼æŠ€è¡“ï¼ç™ºæ˜è€… Ã— æŠ€è¡“ï¼‰
                        G = nx.Graph()

                        # å‡ºé¡˜äººã¨ç”¨é€”ãƒ»æŠ€è¡“
                        for row in result.itertuples():
                            for applicant in row.å‡ºé¡˜äººãƒªã‚¹ãƒˆ:
                                G.add_node(applicant, type="å‡ºé¡˜äºº")
                                for use in row.èª²é¡Œãƒªã‚¹ãƒˆ:
                                    G.add_node(use, type="èª²é¡Œ")
                                    G.add_edge(applicant, use)
                                for tech in row.æŠ€è¡“ãƒªã‚¹ãƒˆ:
                                    G.add_node(tech, type="æŠ€è¡“")
                                    G.add_edge(applicant, tech)

                        # ç™ºæ˜è€…ã¨æŠ€è¡“è¦ç´ 
                        for row in result.itertuples():
                            for inventor in row.ç™ºæ˜è€…ãƒªã‚¹ãƒˆ:
                                G.add_node(inventor, type="ç™ºæ˜è€…")
                                for tech in row.æŠ€è¡“ãƒªã‚¹ãƒˆ:
                                    G.add_node(tech, type="æŠ€è¡“")
                                    G.add_edge(inventor, tech)

                        # ãƒãƒ¼ãƒ‰ä½ç½®è¨ˆç®—
                        pos = nx.spring_layout(G, seed=42)

                        # ãƒ—ãƒ­ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
                        edge_x, edge_y = [], []
                        for edge in G.edges():
                            x0, y0 = pos[edge[0]]
                            x1, y1 = pos[edge[1]]
                            edge_x += [x0, x1, None]
                            edge_y += [y0, y1, None]

                        node_x, node_y, node_text, node_color = [], [], [], []
                        for node in G.nodes():
                            x, y = pos[node]
                            node_x.append(x)
                            node_y.append(y)
                            node_text.append(node)
                            node_type = G.nodes[node]['type']
                            if node_type == "å‡ºé¡˜äºº":
                                node_color.append("lightcoral")
                            elif node_type == "èª²é¡Œ":
                                node_color.append("skyblue")
                            elif node_type == "æŠ€è¡“":
                                node_color.append("lightgreen")
                            elif node_type == "ç™ºæ˜è€…":
                                node_color.append("gold")

                        # Plotlyãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æç”»
                        edge_trace = go.Scatter(
                            x=edge_x, y=edge_y,
                            line=dict(width=0.5, color='#888'),
                            hoverinfo='none',
                            mode='lines')

                        node_trace = go.Scatter(
                            x=node_x, y=node_y,
                            mode='markers+text',
                            text=node_text,
                            textposition="top center",
                            hoverinfo='text',
                            marker=dict(
                                color=node_color,
                                size=12,
                                line_width=2))

                        fig_nt = go.Figure(data=[edge_trace, node_trace],
                                    layout=go.Layout(
                                        title='å‡ºé¡˜äººãƒ»ç™ºæ˜è€… Ã— æŠ€è¡“è¦ç´ ãƒ»èª²é¡Œã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³',
                                        showlegend=False,
                                        hovermode='closest',
                                        margin=dict(b=20,l=5,r=5,t=40),
                                        xaxis=dict(showgrid=False, zeroline=False),
                                        yaxis=dict(showgrid=False, zeroline=False))
                        )
                        st.plotly_chart(fig_nt, use_container_width=True)
                        
                        #åˆ¥ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³
                        #result = _df.dropna(subset=[applicants_list]).copy()
                                        
                        #result["æŠ€è¡“ãƒªã‚¹ãƒˆ"] = result["è§£æ±ºæ‰‹æ®µåˆ†é¡"].apply(split_keywords)
                        #result["å‡ºé¡˜äººãƒªã‚¹ãƒˆ"] = result["å‡ºé¡˜äºº"].apply(split_keywords)
                        #result["ç™ºæ˜è€…ãƒªã‚¹ãƒˆ"] = result["ç™ºæ˜è€…"].apply(split_keywords)

                        # TF-IDF + KMeansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆæŠ€è¡“è¦ç´ ãƒ™ãƒ¼ã‚¹ï¼‰
                        tech_docs = [" ".join(techs) for techs in result["æŠ€è¡“ãƒªã‚¹ãƒˆ"]]
                        vectorizer = TfidfVectorizer()
                        X = vectorizer.fit_transform(tech_docs)
                        kmeans = KMeans(n_clusters=5, random_state=42)
                        result["æŠ€è¡“ã‚¯ãƒ©ã‚¹ã‚¿"] = kmeans.fit_predict(X)

                        # ==========================
                        # å‡ºé¡˜äººãƒ»ç™ºæ˜è€…ã”ã¨ã®ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒé›†è¨ˆ
                        # ==========================

                        # å‡ºé¡˜äºº Ã— æŠ€è¡“ã‚¯ãƒ©ã‚¹ã‚¿
                        applicant_cluster_records = []
                        for row in result.itertuples():
                            for applicant in row.å‡ºé¡˜äººãƒªã‚¹ãƒˆ:
                                applicant_cluster_records.append((applicant, row.æŠ€è¡“ã‚¯ãƒ©ã‚¹ã‚¿))
                        df_applicant_cluster = pd.DataFrame(applicant_cluster_records, columns=["å‡ºé¡˜äºº", "æŠ€è¡“ã‚¯ãƒ©ã‚¹ã‚¿"])
                        df_applicant_cluster_summary = df_applicant_cluster.value_counts().unstack(fill_value=0)

                        # ç™ºæ˜è€… Ã— æŠ€è¡“ã‚¯ãƒ©ã‚¹ã‚¿
                        inventor_cluster_records = []
                        for row in result.itertuples():
                            for inventor in row.ç™ºæ˜è€…ãƒªã‚¹ãƒˆ:
                                inventor_cluster_records.append((inventor, row.æŠ€è¡“ã‚¯ãƒ©ã‚¹ã‚¿))
                        df_inventor_cluster = pd.DataFrame(inventor_cluster_records, columns=["ç™ºæ˜è€…", "æŠ€è¡“ã‚¯ãƒ©ã‚¹ã‚¿"])
                        df_inventor_cluster_summary = df_inventor_cluster.value_counts().unstack(fill_value=0)

                        # ==========================
                        # ã‚¨ãƒƒã‚¸é‡ã¿ä»˜ããƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ï¼ˆå‡ºé¡˜äºº Ã— æŠ€è¡“è¦ç´ ï¼‰
                        # ==========================

                        # å‡ºé¡˜äººã¨æŠ€è¡“ã®å…±èµ·æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                        edge_counter = {}
                        for row in result.itertuples():
                            for applicant in row.å‡ºé¡˜äººãƒªã‚¹ãƒˆ:
                                for tech in row.æŠ€è¡“ãƒªã‚¹ãƒˆ:
                                    pair = (applicant, tech)
                                    edge_counter[pair] = edge_counter.get(pair, 0) + 1

                        # ã‚°ãƒ©ãƒ•æ§‹ç¯‰ï¼ˆé‡ã¿ä»˜ãï¼‰
                        G = nx.Graph()
                        for (applicant, tech), weight in edge_counter.items():
                            G.add_node(applicant, type="å‡ºé¡˜äºº")
                            G.add_node(tech, type="æŠ€è¡“")
                            G.add_edge(applicant, tech, weight=weight)

                        # ãƒãƒ¼ãƒ‰ä½ç½®
                        pos = nx.spring_layout(G, seed=42)

                        # ã‚¨ãƒƒã‚¸ç·šï¼ˆé‡ã¿ã§å¤ªã•èª¿æ•´ï¼‰
                        edge_x, edge_y, edge_width = [], [], []
                        for u, v, data in G.edges(data=True):
                            x0, y0 = pos[u]
                            x1, y1 = pos[v]
                            edge_x += [x0, x1, None]
                            edge_y += [y0, y1, None]
                            edge_width.append(data['weight'])

                        edge_trace = go.Scatter(
                            x=edge_x, y=edge_y,
                            line=dict(width=1, color='gray'),
                            hoverinfo='none',
                            mode='lines'
                        )

                        # ãƒãƒ¼ãƒ‰æç”»
                        node_x, node_y, node_text, node_color = [], [], [], []
                        for node in G.nodes():
                            x, y = pos[node]
                            node_x.append(x)
                            node_y.append(y)
                            node_text.append(node)
                            node_type = G.nodes[node]['type']
                            node_color.append("lightcoral" if node_type == "å‡ºé¡˜äºº" else "lightgreen")

                        node_trace = go.Scatter(
                            x=node_x, y=node_y,
                            mode='markers+text',
                            text=node_text,
                            textposition="top center",
                            hoverinfo='text',
                            marker=dict(color=node_color, size=12, line_width=2)
                        )

                        # å›³ä½œæˆ
                        fig_weighted = go.Figure(data=[edge_trace, node_trace],
                            layout=go.Layout(
                                title='å‡ºé¡˜äºº Ã— æŠ€è¡“è¦ç´ ã®é‡ã¿ä»˜ããƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³',
                                showlegend=False,
                                hovermode='closest',
                                margin=dict(b=20,l=5,r=5,t=40),
                                xaxis=dict(showgrid=False, zeroline=False),
                                yaxis=dict(showgrid=False, zeroline=False))
                        )
                        
                        st.plotly_chart(fig_weighted, use_container_width=True)
                        
                    elif analyze_mode == "ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ—":
                        st.write("å¯¾è±¡ç‰©")    
                        # ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç™ºæ˜ç­‰ã®åç§°ã‚’å–å¾—
                        appnames = _df['å¯¾è±¡ç‰©'].values

                        # Janomeãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’åˆæœŸåŒ–
                        tokenizer = Tokenizer()
                        words = []

                        # åè©ã®é€£ç¶šã‚’æ¤œå‡ºã—ã¦è¤‡åˆåè©ã¨ã—ã¦çµåˆ
                        for appname in appnames:
                            tokens = tokenizer.tokenize(appname)
                            noun_phrase = []  # è¤‡åˆåè©ç”¨ã®ãƒªã‚¹ãƒˆ
                            for token in tokens:
                                if token.part_of_speech.startswith('åè©'):
                                    noun_phrase.append(token.surface)  # åè©ã‚’è¿½åŠ 
                                else:
                                    if noun_phrase:  # åè©ãŒé€£ç¶šã—ã¦ã„ãŸå ´åˆã€çµåˆã—ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ 
                                        words.append("".join(noun_phrase))
                                        noun_phrase = []  # ãƒªã‚»ãƒƒãƒˆ
                            # æœ€å¾Œã«æ®‹ã£ãŸåè©ã‚’è¿½åŠ 
                            if noun_phrase:
                                words.append("".join(noun_phrase))

                        # å˜èªã®å‡ºç¾é »åº¦ã‚’è¨ˆç®—
                        df_words = pd.Series(words).value_counts()
                        word_counts = df_words.to_dict()

                        # å˜èªã®é »åº¦ã‚’æ£’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º
                        fig,ax = plt.subplots(figsize=(20, 13))
                        head_20 = df_words.iloc[:20].copy()
                        ax.barh(y=head_20.index, width=head_20.values, color="orange")
                                            
                        # ã‚°ãƒ©ãƒ•ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ©ãƒ™ãƒ«
                        ax.set_title("é »å‡ºå˜èªã®ãƒˆãƒƒãƒ—20", fontsize=40)
                        ax.set_xlabel("é »åº¦", fontsize=40)
                        ax.set_ylabel("å˜èª", fontsize=40)

                        # yè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’èª¿æ•´
                        #ax.set_yticks(head_20.index)
                        #ax.set_yticklabels(head_20.index, rotation=0, fontsize=30)
                        ax.tick_params(axis='y', labelsize=30)
                        ax.tick_params(axis='x', labelsize=30)
                        
                        st.pyplot(fig)
                        
                        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’æŒ‡å®š
                        font_path1 = "./font/NotoSansJP-Regular.ttf"  # é©åˆ‡ãªãƒ‘ã‚¹ã‚’æŒ‡å®š

                        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ
                        wordcloud = WordCloud(
                            background_color='white', 
                            width=800, 
                            height=600, 
                            font_path=font_path1
                        )

                        wordcloud.generate_from_frequencies(word_counts)

                        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’è¡¨ç¤º
                        plt.figure(figsize=(10, 8))
                        plt.imshow(wordcloud, interpolation='bilinear')
                        plt.axis('off')

                        # Streamlitã§è¡¨ç¤º
                        st.pyplot(plt)
                        st.write("è«‹æ±‚é …ã®å¯¾è±¡")
                        appnames = _df['è«‹æ±‚é …ã®å¯¾è±¡'].values

                        # Janomeãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’åˆæœŸåŒ–
                        tokenizer = Tokenizer()
                        words = []

                        # åè©ã®é€£ç¶šã‚’æ¤œå‡ºã—ã¦è¤‡åˆåè©ã¨ã—ã¦çµåˆ
                        for appname in appnames:
                            tokens = tokenizer.tokenize(appname)
                            noun_phrase = []  # è¤‡åˆåè©ç”¨ã®ãƒªã‚¹ãƒˆ
                            for token in tokens:
                                if token.part_of_speech.startswith('åè©'):
                                    noun_phrase.append(token.surface)  # åè©ã‚’è¿½åŠ 
                                else:
                                    if noun_phrase:  # åè©ãŒé€£ç¶šã—ã¦ã„ãŸå ´åˆã€çµåˆã—ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ 
                                        words.append("".join(noun_phrase))
                                        noun_phrase = []  # ãƒªã‚»ãƒƒãƒˆ
                            # æœ€å¾Œã«æ®‹ã£ãŸåè©ã‚’è¿½åŠ 
                            if noun_phrase:
                                words.append("".join(noun_phrase))

                        # å˜èªã®å‡ºç¾é »åº¦ã‚’è¨ˆç®—
                        df_words = pd.Series(words).value_counts()
                        word_counts = df_words.to_dict()

                        # å˜èªã®é »åº¦ã‚’æ£’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º
                        fig,ax = plt.subplots(figsize=(20, 13))
                        head_20 = df_words.iloc[:20].copy()
                        ax.barh(y=head_20.index, width=head_20.values, color="orange")
                                            
                        # ã‚°ãƒ©ãƒ•ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ©ãƒ™ãƒ«
                        ax.set_title("é »å‡ºå˜èªã®ãƒˆãƒƒãƒ—20", fontsize=40)
                        ax.set_xlabel("é »åº¦", fontsize=40)
                        ax.set_ylabel("å˜èª", fontsize=40)

                        # yè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’èª¿æ•´
                        #ax.set_yticks(head_20.index)
                        #ax.set_yticklabels(head_20.index, rotation=0, fontsize=30)
                        ax.tick_params(axis='y', labelsize=30)
                        ax.tick_params(axis='x', labelsize=30)
                        
                        st.pyplot(fig)
                        


                        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ
                        wordcloud = WordCloud(
                            background_color='white', 
                            width=800, 
                            height=600, 
                            font_path=font_path1
                        )

                        wordcloud.generate_from_frequencies(word_counts)

                        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’è¡¨ç¤º
                        plt.figure(figsize=(10, 8))
                        plt.imshow(wordcloud, interpolation='bilinear')
                        plt.axis('off')
                        
                        

                        # Streamlitã§è¡¨ç¤º
                        st.pyplot(plt)
                        st.write("æŠ€è¡“é–‹ç™ºèª²é¡Œåˆ†é¡")
                        appnames = _df['æŠ€è¡“é–‹ç™ºèª²é¡Œåˆ†é¡'].values

                        # Janomeãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’åˆæœŸåŒ–
                        tokenizer = Tokenizer()
                        words = []

                        # åè©ã®é€£ç¶šã‚’æ¤œå‡ºã—ã¦è¤‡åˆåè©ã¨ã—ã¦çµåˆ
                        for appname in appnames:
                            tokens = tokenizer.tokenize(appname)
                            noun_phrase = []  # è¤‡åˆåè©ç”¨ã®ãƒªã‚¹ãƒˆ
                            for token in tokens:
                                if token.part_of_speech.startswith('åè©'):
                                    noun_phrase.append(token.surface)  # åè©ã‚’è¿½åŠ 
                                else:
                                    if noun_phrase:  # åè©ãŒé€£ç¶šã—ã¦ã„ãŸå ´åˆã€çµåˆã—ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ 
                                        words.append("".join(noun_phrase))
                                        noun_phrase = []  # ãƒªã‚»ãƒƒãƒˆ
                            # æœ€å¾Œã«æ®‹ã£ãŸåè©ã‚’è¿½åŠ 
                            if noun_phrase:
                                words.append("".join(noun_phrase))

                        # å˜èªã®å‡ºç¾é »åº¦ã‚’è¨ˆç®—
                        df_words = pd.Series(words).value_counts()
                        word_counts = df_words.to_dict()

                        # å˜èªã®é »åº¦ã‚’æ£’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º
                        fig,ax = plt.subplots(figsize=(20, 13))
                        head_20 = df_words.iloc[:20].copy()
                        ax.barh(y=head_20.index, width=head_20.values, color="orange")
                                            
                        # ã‚°ãƒ©ãƒ•ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ©ãƒ™ãƒ«
                        ax.set_title("é »å‡ºå˜èªã®ãƒˆãƒƒãƒ—20", fontsize=40)
                        ax.set_xlabel("é »åº¦", fontsize=40)
                        ax.set_ylabel("å˜èª", fontsize=40)

                        # yè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’èª¿æ•´
                        #ax.set_yticks(head_20.index)
                        #ax.set_yticklabels(head_20.index, rotation=0, fontsize=30)
                        ax.tick_params(axis='y', labelsize=30)
                        ax.tick_params(axis='x', labelsize=30)
                        
                        st.pyplot(fig)
                        


                        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ
                        wordcloud = WordCloud(
                            background_color='white', 
                            width=800, 
                            height=600, 
                            font_path=font_path1
                        )

                        wordcloud.generate_from_frequencies(word_counts)

                        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’è¡¨ç¤º
                        plt.figure(figsize=(10, 8))
                        plt.imshow(wordcloud, interpolation='bilinear')
                        plt.axis('off')
                        
                        

                        # Streamlitã§è¡¨ç¤º
                        st.pyplot(plt)
                        
                        st.write("è§£æ±ºæ‰‹æ®µåˆ†é¡")
                        appnames = _df['è§£æ±ºæ‰‹æ®µåˆ†é¡'].values

                        # Janomeãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’åˆæœŸåŒ–
                        tokenizer = Tokenizer()
                        words = []

                        # åè©ã®é€£ç¶šã‚’æ¤œå‡ºã—ã¦è¤‡åˆåè©ã¨ã—ã¦çµåˆ
                        for appname in appnames:
                            tokens = tokenizer.tokenize(appname)
                            noun_phrase = []  # è¤‡åˆåè©ç”¨ã®ãƒªã‚¹ãƒˆ
                            for token in tokens:
                                if token.part_of_speech.startswith('åè©'):
                                    noun_phrase.append(token.surface)  # åè©ã‚’è¿½åŠ 
                                else:
                                    if noun_phrase:  # åè©ãŒé€£ç¶šã—ã¦ã„ãŸå ´åˆã€çµåˆã—ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ 
                                        words.append("".join(noun_phrase))
                                        noun_phrase = []  # ãƒªã‚»ãƒƒãƒˆ
                            # æœ€å¾Œã«æ®‹ã£ãŸåè©ã‚’è¿½åŠ 
                            if noun_phrase:
                                words.append("".join(noun_phrase))

                        # å˜èªã®å‡ºç¾é »åº¦ã‚’è¨ˆç®—
                        df_words = pd.Series(words).value_counts()
                        word_counts = df_words.to_dict()

                        # å˜èªã®é »åº¦ã‚’æ£’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º
                        fig,ax = plt.subplots(figsize=(20, 13))
                        head_20 = df_words.iloc[:20].copy()
                        ax.barh(y=head_20.index, width=head_20.values, color="orange")
                                            
                        # ã‚°ãƒ©ãƒ•ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ©ãƒ™ãƒ«
                        ax.set_title("é »å‡ºå˜èªã®ãƒˆãƒƒãƒ—20", fontsize=40)
                        ax.set_xlabel("é »åº¦", fontsize=40)
                        ax.set_ylabel("å˜èª", fontsize=40)

                        # yè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’èª¿æ•´
                        #ax.set_yticks(head_20.index)
                        #ax.set_yticklabels(head_20.index, rotation=0, fontsize=30)
                        ax.tick_params(axis='y', labelsize=30)
                        ax.tick_params(axis='x', labelsize=30)
                        
                        st.pyplot(fig)
                        


                        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ
                        wordcloud = WordCloud(
                            background_color='white', 
                            width=800, 
                            height=600, 
                            font_path=font_path1
                        )

                        wordcloud.generate_from_frequencies(word_counts)

                        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’è¡¨ç¤º
                        plt.figure(figsize=(10, 8))
                        plt.imshow(wordcloud, interpolation='bilinear')
                        plt.axis('off')
                        
                        

                        # Streamlitã§è¡¨ç¤º
                        st.pyplot(plt)

                    elif analyze_mode == "ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆ":
                         # æ¬ æå€¤ã‚’å‰Šé™¤
                        applicants_list = st.selectbox("å‡ºé¡˜äººã«é–¢é€£ã™ã‚‹é …ç›®ã‚’é¸æŠã€‚", df_columns)

                        # æ¬ æé™¤å»
                        df_clean = _df.dropna(subset=[applicants_list]).copy()
                        df_clean = df_clean.dropna(subset=["å¯¾è±¡ç‰©"]).copy()

                        # å¯¾è±¡ç‰©ã‚’ã‚»ãƒŸã‚³ãƒ­ãƒ³åˆ†å‰²ã—ã¦ãƒªã‚¹ãƒˆåŒ–
                        df_clean["object_list"] = df_clean["å¯¾è±¡ç‰©"].apply(lambda x: [a.strip() for a in str(x).split(";")])

                        # ä¸€æ„ãªå¯¾è±¡ç‰©ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—
                        flattened_object = [obj for sublist in df_clean["object_list"] for obj in sublist]
                        unique_values = sorted(pd.unique(flattened_object))

                        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é¸ã°ã›ã‚‹
                        company = st.multiselect("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å¯è¦–åŒ–ã™ã‚‹å¯¾è±¡ç‰©ã‚’é¸æŠã—ã¦ãã ã•ã„", unique_values, default=unique_values)

                        # éƒ¨åˆ†ä¸€è‡´ãƒ•ã‚£ãƒ«ã‚¿é–¢æ•°ï¼ˆå¯¾è±¡ç‰©ã«1ã¤ã§ã‚‚companyã®ã©ã‚Œã‹ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°OKï¼‰
                        def object_partial_match_filter(object_list):
                            return any(any(c in obj for obj in object_list) for c in company)

                        # ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
                        result = df_clean[df_clean["object_list"].apply(object_partial_match_filter)]
                        
                        
                        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®åˆ†å‰²å‡¦ç†
                        def split_keywords(text):
                            if pd.isna(text) or not isinstance(text, str):
                                return []
                            return [kw.strip() for kw in text.split(",") if kw.strip()]

                        result["èª²é¡Œãƒªã‚¹ãƒˆ"] = result["æŠ€è¡“é–‹ç™ºèª²é¡Œåˆ†é¡"].apply(split_keywords)
                        result["æŠ€è¡“ãƒªã‚¹ãƒˆ"] = result["è§£æ±ºæ‰‹æ®µåˆ†é¡"].apply(split_keywords)
                        
                        bubble_data = []
                        for _, row in result.iterrows():
                            company_name = row[applicants_list]
                            for use in row.èª²é¡Œãƒªã‚¹ãƒˆ:
                                for tech in row.æŠ€è¡“ãƒªã‚¹ãƒˆ:
                                    bubble_data.append({
                                        'æŠ€è¡“': tech,
                                        'èª²é¡Œ': use,
                                        'å‡ºé¡˜äºº': company_name
                                    })

                        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›ã—ã¦é›†è¨ˆ
                        bubble_df = pd.DataFrame(bubble_data)
                        bubble_count = bubble_df.groupby(['æŠ€è¡“', 'èª²é¡Œ']).size().reset_index(name='å‡ºé¡˜ä»¶æ•°')

                        # ä¸Šä½ã®æŠ€è¡“ã¨èª²é¡Œã‚’å–å¾—ï¼ˆã‚ã¾ã‚Šã«å¤šã„ã¨è¦‹ã¥ã‚‰ããªã‚‹ãŸã‚ï¼‰
                        top_techs = bubble_df['æŠ€è¡“'].value_counts().nlargest(15).index.tolist()
                        top_issues = bubble_df['èª²é¡Œ'].value_counts().nlargest(15).index.tolist()

                        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                        filtered_bubble = bubble_count[
                            bubble_count['æŠ€è¡“'].isin(top_techs) & 
                            bubble_count['èª²é¡Œ'].isin(top_issues)
                        ]

                        # ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆä½œæˆ
                        fig_bubble = go.Figure()

                        fig_bubble.add_trace(go.Scatter(
                            x=filtered_bubble['æŠ€è¡“'],
                            y=filtered_bubble['èª²é¡Œ'],
                            mode='markers',
                            marker=dict(
                                size=filtered_bubble['å‡ºé¡˜ä»¶æ•°'] * 10,  # ã‚µã‚¤ã‚ºã¯é©å®œèª¿æ•´
                                sizemode='area',
                                sizeref=2. * max(filtered_bubble['å‡ºé¡˜ä»¶æ•°']) / (40.**2),  # ãƒãƒ–ãƒ«ã‚µã‚¤ã‚ºèª¿æ•´
                                sizemin=4,
                                color=filtered_bubble['å‡ºé¡˜ä»¶æ•°'],
                                colorscale='Viridis',
                                showscale=True,
                                colorbar=dict(title='å‡ºé¡˜ä»¶æ•°')
                            ),
                            text=[f'æŠ€è¡“: {tech}<br>èª²é¡Œ: {use}<br>å‡ºé¡˜æ•°: {count}' 
                                for tech, use, count in zip(filtered_bubble['æŠ€è¡“'], filtered_bubble['èª²é¡Œ'], filtered_bubble['å‡ºé¡˜ä»¶æ•°'])],
                            hoverinfo='text'
                        ))

                        fig_bubble.update_layout(
                            title='æŠ€è¡“-èª²é¡Œã®ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆï¼ˆãƒãƒ–ãƒ«ã‚µã‚¤ã‚ºï¼šå‡ºé¡˜ä»¶æ•°ï¼‰',
                            xaxis=dict(
                                title='æŠ€è¡“',
                                categoryorder='total ascending'
                            ),
                            yaxis=dict(
                                title='èª²é¡Œ',
                                categoryorder='total ascending'
                            ),
                            height=800,
                            width=900
                        )

                        # ãƒ—ãƒ­ãƒƒãƒˆè¡¨ç¤º
                        st.plotly_chart(fig_bubble, use_container_width=True)

                        # å‡ºé¡˜äººåˆ¥ã®é›†è¨ˆã‚‚è¦‹ã›ã‚‹
                        st.subheader("å‡ºé¡˜äººåˆ¥ã®é›†è¨ˆ")
                        company_counts = bubble_df['å‡ºé¡˜äºº'].value_counts().reset_index()
                        company_counts.columns = ['å‡ºé¡˜äºº', 'å‡ºé¡˜ä»¶æ•°']
                        st.dataframe(company_counts)

                
                except Exception as e:
                    st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€éš›ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    
        if __name__ == "__main__":
            st_rag_langgraph()
    
    elif mode == "å¯¾æ¯”è¡¨ä½œæˆã‚·ã‚¹ãƒ†ãƒ ":
        

        # --- Driveé–¢é€£é–¢æ•° ---
        def upload_to_drive(folder_id, file_path, mime_type='application/pdf'):
            unique_name = f"{os.path.splitext(os.path.basename(file_path))[0]}_{int(time.time())}_{uuid.uuid4().hex[:6]}.pdf"
            media = MediaFileUpload(file_path, mimetype=mime_type)
            file_metadata = {'name': unique_name, 'parents': [folder_id]}
            uploaded = drive_service.files().create(body=file_metadata, media_body=media, fields='id,name').execute()
            return uploaded['id'], uploaded['name']

        def delete_from_drive(file_id):
            try:
                drive_service.files().delete(fileId=file_id).execute()
            except HttpError as e:
                print(f"å‰Šé™¤å¤±æ•—: {e}")

        def convert_pdf_to_doc(file_id, output_folder_id):
            copied = drive_service.files().copy(fileId=file_id, body={
                'mimeType': 'application/vnd.google-apps.document',
                'parents': [output_folder_id]
            }).execute()
            return copied['id']

        def download_doc_as_docx(file_id, local_path):
            try:
                request = drive_service.files().export_media(fileId=file_id,
                                                            mimeType='application/vnd.openxmlformats-officedocument.wordprocessingml.document')
                with open(local_path, 'wb') as f:
                    f.write(request.execute())
                return local_path
            except HttpError as error:
                print(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {error}")
                return None

        # --- OCRå‰å‡¦ç† ---
        def preprocess_image_for_ocr(pil_image):
            img = np.array(pil_image.convert("L"))
            img = cv2.resize(img, None, fx=2, fy=2, interpolation=cv2.INTER_LINEAR)
            img = cv2.GaussianBlur(img, (3, 3), 0)
            img = cv2.equalizeHist(img)
            return Image.fromarray(img)

        # --- OCRæŠ½å‡ºé–¢æ•° ---
        def extract_text_from_docx_images(docx_path):
            doc = Document(docx_path)
            image_texts = []
            for rel in doc.part._rels:
                rel_obj = doc.part._rels[rel]
                if "image" in rel_obj.reltype:
                    image_data = rel_obj.target_part.blob
                    pil_image = Image.open(BytesIO(image_data)).convert("RGB")
                    buffered = BytesIO()
                    pil_image.save(buffered, format="PNG")
                    content = buffered.getvalue()

                    image = vision.Image(content=content)
                    response = vision_client.document_text_detection(image=image)
                    texts = response.text_annotations
                    if texts:
                        image_texts.append(texts[0].description.strip())
            return "\n".join(image_texts)

        # --- ãƒãƒ£ãƒ³ã‚¯åˆ†å‰² ---
        def split_text_into_chunks(text, chunk_size=1000, overlap=100):
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=overlap,
                separators=["\n\n", "\n", ".", "ã€‚", "ã€", " "]
            )
            return splitter.split_text(text)

        # --- FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ ---
        def create_faiss_index(text_chunks, embedding_model):
            docs = [LangChainDocument(page_content=chunk) for chunk in text_chunks]
            return FAISS.from_documents(docs, embedding_model)

        # --- è«‹æ±‚é …ã‚’æ„å‘³å˜ä½ã§åˆ†å‰² ---
        def split_claims_into_chunks(claims_list, llm):
            split_results = []
            for claim in claims_list:
                prompt = ChatPromptTemplate.from_template("""
        ã‚ãªãŸã¯ç‰¹è¨±è«‹æ±‚é …ã‚’è‡ªç„¶ãªæ„å‘³ã®ã¾ã¨ã¾ã‚Šã”ã¨ã«åˆ†å‰²ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
        ä»¥ä¸‹ã®è«‹æ±‚é …ã‚’100ã€œ200æ–‡å­—ç¨‹åº¦ã§è‡ªç„¶ã«åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚

        ã€è«‹æ±‚é …ã€‘:
        {claim}

        ã€å‡ºåŠ›å½¢å¼ã€‘:
        - åˆ†å‰²1: ...
        - åˆ†å‰²2: ...
        """)
                chain = prompt | llm
                try:
                    response = chain.invoke({"claim": claim})
                    split_text = response.content.strip()
                    split_texts = []
                    for line in split_text.split("\n"):
                        if "- åˆ†å‰²" in line:
                            _, text = line.split(":", 1)
                            split_texts.append(text.strip())
                    split_results.append(split_texts)
                except Exception as e:
                    print(f"åˆ†å‰²ã‚¨ãƒ©ãƒ¼: {e}")
                    split_results.append([claim])
            return split_results
        
        def main():
            # --- Streamlitã‚¢ãƒ—ãƒªæœ¬ä½“ ---
            
            st.title("ç‰¹è¨±å¯¾æ¯”è¡¨ä½œæˆãƒ„ãƒ¼ãƒ«ï¼ˆFAISSï¼‹ã‚¹ã‚³ã‚¢ï¼‹å…±é€šç‚¹æ•´ç†ç‰ˆï¼‰")
            st.image("./fig/å¯¾æ¯”è¡¨ä½œæˆã‚·ã‚¹ãƒ†ãƒ 1.jpg", use_container_width=True)
            st.image("./fig/å¯¾æ¯”è¡¨ä½œæˆã‚·ã‚¹ãƒ†ãƒ 2.jpg", use_container_width=True)
            st.image("./fig/å¯¾æ¯”è¡¨ä½œæˆã‚·ã‚¹ãƒ†ãƒ 3.jpg", use_container_width=True)

            uploaded_excel = st.file_uploader("è«‹æ±‚é …ãƒªã‚¹ãƒˆï¼ˆExcelï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx"])
            uploaded_pdfs = st.file_uploader("æ¯”è¼ƒå¯¾è±¡ã®ç‰¹è¨±PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°å¯ï¼‰", type=["pdf"], accept_multiple_files=True)


            if uploaded_excel and uploaded_pdfs:
                claims_df = pd.read_excel(uploaded_excel)
                claim_column = st.selectbox("è«‹æ±‚é …ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹åˆ—ã‚’é¸ã‚“ã§ãã ã•ã„", claims_df.columns)

                if st.button("âœ¨ å¯¾æ¯”è¡¨ä½œæˆã‚¹ã‚¿ãƒ¼ãƒˆï¼"):
                    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.0)
                    embedding_model = OpenAIEmbeddings(model="text-embedding-ada-002")
                    progress = st.progress(0.1)

                    # --- ã‚¹ãƒ†ãƒƒãƒ—1: è«‹æ±‚é …ã‚’åˆ†å‰² ---
                    claim_texts = claims_df[claim_column].tolist()
                    split_claims = split_claims_into_chunks(claim_texts, llm)

                    split_rows = []
                    for idx, splits in enumerate(split_claims):
                        for split_idx, split_text in enumerate(splits, 1):
                            split_rows.append({
                                "å…ƒè«‹æ±‚é …No.": idx + 1,
                                "å…ƒè«‹æ±‚é …æœ¬æ–‡": claim_texts[idx],
                                "åˆ†å‰²No.": split_idx,
                                "åˆ†å‰²æ–‡": split_text,
                            })
                    split_df = pd.DataFrame(split_rows)

                    # --- ã‚¹ãƒ†ãƒƒãƒ—2: PDFã”ã¨ã«å‡¦ç† ---
                    for pdf_file in uploaded_pdfs:
                        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                            tmp.write(pdf_file.read())
                            tmp_path = tmp.name

                        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                        file_id, file_name = upload_to_drive(GOOGLE_DRIVE_FOLDER_ID, tmp_path)

                        # PDFâ†’Google Docså¤‰æ›
                        doc_id = convert_pdf_to_doc(file_id, GOOGLE_DRIVE_FOLDER_ID)

                        # DOCXãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                        docx_local_path = download_doc_as_docx(doc_id, tmp_path.replace(".pdf", ".docx"))
                        
                        text = extract_text_from_docx_images(docx_local_path)

                        delete_from_drive(file_id)
                        delete_from_drive(doc_id)
                        os.remove(tmp_path)
                        os.remove(docx_local_path)

                        if not text.strip():
                            st.warning(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ« {pdf_file.name} ã¯OCRçµæœãŒç©ºã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                            continue

                        chunks = split_text_into_chunks(text, chunk_size=1000)
                        if not chunks:
                            st.warning(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ« {pdf_file.name} ã¯ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                            continue

                        faiss_index = create_faiss_index(chunks, embedding_model)

                        # 3åˆ—ã‚»ãƒƒãƒˆä½œæˆ
                        result_col_text = f"{pdf_file.name}_é¡ä¼¼åŸæ–‡"
                        result_col_score = f"{pdf_file.name}_ã‚¹ã‚³ã‚¢"
                        result_col_summary = f"{pdf_file.name}_å…±é€šç‚¹æ•´ç†"

                        split_df[result_col_text] = ""
                        split_df[result_col_score] = ""
                        split_df[result_col_summary] = ""

                        for idx, row in split_df.iterrows():
                            query = row["åˆ†å‰²æ–‡"]
                            results = faiss_index.similarity_search_with_score(query, k=1)

                            if results:
                                best_doc, best_score = results[0]
                                best_text = best_doc.page_content

                                split_df.at[idx, result_col_text] = best_text
                                split_df.at[idx, result_col_score] = round(best_score, 3)

                                prompt = ChatPromptTemplate.from_template("""
            ä»¥ä¸‹ã®è«‹æ±‚é …åˆ†å‰²æ–‡ã¨ã€æŠ½å‡ºã•ã‚ŒãŸç‰¹è¨±æœ¬æ–‡ã‚’æ¯”è¼ƒã—ã€
            - æŠ€è¡“åˆ†é‡
            - ææ–™
            - å·¥ç¨‹
            - ç‰¹å¾´çš„ãªæ¡ä»¶
            ã‚’ä¸å¯§ã«åˆ—æŒ™ã—ã¦ãã ã•ã„ã€‚

            ã€è«‹æ±‚é …åˆ†å‰²æ–‡ã€‘:
            {split_text}

            ã€æŠ½å‡ºã•ã‚ŒãŸç‰¹è¨±æœ¬æ–‡ã€‘:
            {best_text}

            ã€å‡ºåŠ›å½¢å¼ã€‘:
            - æŠ€è¡“åˆ†é‡: ...
            - ææ–™: ...
            - å·¥ç¨‹: ...
            - æ¡ä»¶: ...
            """)
                                chain = prompt | llm

                                try:
                                    response = chain.invoke({"split_text": query, "best_text": best_text})
                                    split_df.at[idx, result_col_summary] = response.content.strip()
                                except Exception as e:
                                    split_df.at[idx, result_col_summary] = f"ã‚¨ãƒ©ãƒ¼: {e}"

                            else:
                                split_df.at[idx, result_col_text] = "é¡ä¼¼ãªã—"
                                split_df.at[idx, result_col_score] = ""
                                split_df.at[idx, result_col_summary] = "æ•´ç†å¯¾è±¡ãªã—"

                        st.progress((uploaded_pdfs.index(pdf_file)+1) / len(uploaded_pdfs), text=f"{uploaded_pdfs.index(pdf_file)+1}/{len(uploaded_pdfs)}ãƒ•ã‚¡ã‚¤ãƒ«ç›® å‡¦ç†ä¸­")
                        
                    progress.progress(0.8)
                    # --- ã‚¹ãƒ†ãƒƒãƒ—3: å‡ºåŠ›
                    output = BytesIO()
                    with pd.ExcelWriter(output, engine='openpyxl') as writer:
                        claims_df.to_excel(writer, index=False, sheet_name="åŸæ–‡")
                        split_df.to_excel(writer, index=False, sheet_name="åˆ†å‰²å¾Œï¼‹å¯¾æ¯”çµæœ")

                    st.success("âœ… å®Œäº†ã—ã¾ã—ãŸï¼")
                    progress.progress(1.0)
                    st.download_button(
                        label="ğŸ“¥ ã‚¨ã‚¯ã‚»ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=output.getvalue(),
                        file_name="è«‹æ±‚é …å¯¾æ¯”çµæœ_FAISS_3åˆ—ç‰ˆ.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )

        if __name__ == "__main__":
            main()


